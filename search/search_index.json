{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CleanRL Overview CleanRL is a Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features. The implementation is clean and simple, yet we can scale it to run thousands of experiments using AWS Batch. The highlight features of CleanRL are: Single-file Implementation Every detail about an algorithm is put into the algorithm's own file. Therefore, it's easier for you to fully understand an algorithm and do research with it. Benchmarked Implementation on 7+ algorithms and 34+ games Tensorboard Logging Local Reproducibility via Seeding Videos of Gameplay Capturing Experiment Management with Weights and Biases Cloud Integration with Docker and AWS You can read more about CleanRL in our technical paper and documentation . Good luck have fun \ud83d\ude80 Citing CleanRL If you use CleanRL in your work, please cite our technical paper : @article { huang2021cleanrl , title = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms} , author = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga} , year = {2021} , eprint = {2111.08819} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"Overview"},{"location":"#cleanrl","text":"","title":"CleanRL"},{"location":"#overview","text":"CleanRL is a Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features. The implementation is clean and simple, yet we can scale it to run thousands of experiments using AWS Batch. The highlight features of CleanRL are: Single-file Implementation Every detail about an algorithm is put into the algorithm's own file. Therefore, it's easier for you to fully understand an algorithm and do research with it. Benchmarked Implementation on 7+ algorithms and 34+ games Tensorboard Logging Local Reproducibility via Seeding Videos of Gameplay Capturing Experiment Management with Weights and Biases Cloud Integration with Docker and AWS You can read more about CleanRL in our technical paper and documentation . Good luck have fun \ud83d\ude80","title":"Overview"},{"location":"#citing-cleanrl","text":"If you use CleanRL in your work, please cite our technical paper : @article { huang2021cleanrl , title = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms} , author = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga} , year = {2021} , eprint = {2111.08819} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"Citing CleanRL"},{"location":"community/","text":"We have a Discord Community for support. Feel free to ask questions. Posting in Github Issues and PRs are also welcome. Also our past video recordings are available at YouTube Related Resources Deep Reinforcement Learning With TensorFlow 2.1 minimalRL - PyTorch Deep-Reinforcement-Learning-Hands-On Stable Baselines3 PyTorch implementation of Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO), Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (ACKTR) and Generative Adversarial Imitation Learning (GAIL). Reinforcement-Implementation","title":"Community"},{"location":"community/#related-resources","text":"Deep Reinforcement Learning With TensorFlow 2.1 minimalRL - PyTorch Deep-Reinforcement-Learning-Hands-On Stable Baselines3 PyTorch implementation of Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO), Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (ACKTR) and Generative Adversarial Imitation Learning (GAIL). Reinforcement-Implementation","title":"Related Resources"},{"location":"contribution/","text":"Thank you for being interested in contributing to our project. All kinds of contributions are welcome. Below are some steps to help you get started: Join our discord channel to say hi!! \ud83d\udc4b Pick something you want to work at and let us know on slack. You could Tackle issues with the help wanted flag Bug fixes and various improvements on existing algorithms Contribute to the Open RL Benchmark You could add new algorithms or new games to be featured in the Open RL Benchmark Free free to contact me (Costa) directly on slack. I will add you to the CleanRL's Team at Weight and Biases. Your experiments can be featured on the Open RL Benchmark . Submit a PR and get it merged! \ud83c\udf87 Good luck and have fun!","title":"Contribution"},{"location":"made-with-cleanrl/","text":"Made with CleanRL CleanRL has become an increasingly popular deep reinforcement learning library, especially among practitioners who prefer more customizable code. Since its debut in July 2019, CleanRL has supported many open source projects and publications. Below are some highlight projects and publications made with CleanRL. Feel free to edit this list if your project or paper has used CleanRL. Publications An Empirical Investigation of Early Stopping Optimizations in Proximal Policy Optimization , Dossa, R., Huang, S., Onta\u00f1\u00f3n, S., Matsubara, T., IEEE Access, 2021 Gym-\u03bcRTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning , Huang, S., Onta\u00f1\u00f3n, S., Bamford, C., Grela, L., IEEE Conference on Games 2021 Measuring Generalization of Deep Reinforcement Learning Applied to Real-time Strategy Games , Huang, S., Onta\u00f1\u00f3n, S., AAAI 2021 Reinforcement Learning in Games Workshop Griddly: A platform for AI research in games , Bamford, C., Huang, S., Lucas, S., AAAI 2021 Reinforcement Learning in Games Workshop Action Guidance: Getting the Best of Sparse Rewards and Shaped Rewards for Real-time Strategy Games , Huang, S., Onta\u00f1\u00f3n, S., AIIDE Workshop on Artificial Intelligence for Strategy Games , October 2020 A Closer Look at Invalid Action Masking in Policy Gradient Algorithms , Huang, S., Onta\u00f1\u00f3n, S., Preprint. Comparing Observation and Action Representations for Reinforcement Learning in \u00b5RTS , Huang, S., Onta\u00f1\u00f3n, S., AIIDE Workshop on Artificial Intelligence for Strategy Games , October 2019","title":"Made with CleanRL"},{"location":"made-with-cleanrl/#made-with-cleanrl","text":"CleanRL has become an increasingly popular deep reinforcement learning library, especially among practitioners who prefer more customizable code. Since its debut in July 2019, CleanRL has supported many open source projects and publications. Below are some highlight projects and publications made with CleanRL. Feel free to edit this list if your project or paper has used CleanRL.","title":"Made with CleanRL"},{"location":"made-with-cleanrl/#publications","text":"An Empirical Investigation of Early Stopping Optimizations in Proximal Policy Optimization , Dossa, R., Huang, S., Onta\u00f1\u00f3n, S., Matsubara, T., IEEE Access, 2021 Gym-\u03bcRTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning , Huang, S., Onta\u00f1\u00f3n, S., Bamford, C., Grela, L., IEEE Conference on Games 2021 Measuring Generalization of Deep Reinforcement Learning Applied to Real-time Strategy Games , Huang, S., Onta\u00f1\u00f3n, S., AAAI 2021 Reinforcement Learning in Games Workshop Griddly: A platform for AI research in games , Bamford, C., Huang, S., Lucas, S., AAAI 2021 Reinforcement Learning in Games Workshop Action Guidance: Getting the Best of Sparse Rewards and Shaped Rewards for Real-time Strategy Games , Huang, S., Onta\u00f1\u00f3n, S., AIIDE Workshop on Artificial Intelligence for Strategy Games , October 2020 A Closer Look at Invalid Action Masking in Policy Gradient Algorithms , Huang, S., Onta\u00f1\u00f3n, S., Preprint. Comparing Observation and Action Representations for Reinforcement Learning in \u00b5RTS , Huang, S., Onta\u00f1\u00f3n, S., AIIDE Workshop on Artificial Intelligence for Strategy Games , October 2019","title":"Publications"},{"location":"open-rl-benchmark/","text":"","title":"Open RL Benchmark"},{"location":"rl-algorithms/","text":"Below are the implemented algorithms and their brief descriptions. [x] Deep Q-Learning (DQN) dqn.py For discrete action space. dqn_atari.py For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. [x] Categorical DQN (C51) c51.py For discrete action space. c51_atari.py For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. c51_atari_visual.py Adds return and q-values visulization for dqn_atari.py . [x] Proximal Policy Gradient (PPO) All of the PPO implementations below are augmented with some code-level optimizations. See https://costa.sh/blog-the-32-implementation-details-of-ppo.html for more details ppo.py For discrete action space. ppo_continuous_action.py For continuous action space. Also implemented Mujoco-specific code-level optimizations ppo_atari.py For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. [x] Soft Actor Critic (SAC) sac_continuous_action.py For continuous action space. [x] Deep Deterministic Policy Gradient (DDPG) ddpg_continuous_action.py For continuous action space. [x] Twin Delayed Deep Deterministic Policy Gradient (TD3) td3_continuous_action.py For continuous action space.","title":"Rl algorithms"},{"location":"advanced/resume-training/","text":"Resume Training A common question we get asked is how to set up model checkpoints to continue training. In this document, we take this PPO example to explain that question. Save model checkpoints The first step is to save models periodically. By default, we save the model to wandb . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 num_updates = args . total_timesteps // args . batch_size CHECKPOINT_FREQUENCY = 50 starting_update = 1 for update in range ( starting_update , num_updates + 1 ): # ... do rollouts and train models if args . track : # make sure to tune `CHECKPOINT_FREQUENCY` # so models are not saved too frequently if update % CHECKPOINT_FREQUENCY == 0 : torch . save ( agent . state_dict (), f \" { wandb . run . dir } /agent.pt\" ) wandb . save ( f \" { wandb . run . dir } /agent.pt\" , policy = \"now\" ) Then we could run the following to train our agents python ppo_gridnet.py --prod-mode --capture-video If the training was terminated early, we can still see the last updated model agent.pt in W&B like in this URL https://wandb.ai/costa-huang/cleanRL/runs/21421tda/files or as follows Resume training The second step is to automatically download the agent.pt from the URL above and resume training as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 num_updates = args . total_timesteps // args . batch_size CHECKPOINT_FREQUENCY = 50 starting_update = 1 if args . track and wandb . run . resumed : starting_update = run . summary . get ( \"charts/update\" ) + 1 global_step = starting_update * args . batch_size api = wandb . Api () run = api . run ( f \" { run . entity } / { run . project } / { run . id } \" ) model = run . file ( \"agent.pt\" ) model . download ( f \"models/ { experiment_name } /\" ) agent . load_state_dict ( torch . load ( f \"models/ { experiment_name } /agent.pt\" , map_location = device )) agent . eval () print ( f \"resumed at update { starting_update } \" ) for update in range ( starting_update , num_updates + 1 ): # ... do rollouts and train models if args . track : # make sure to tune `CHECKPOINT_FREQUENCY` # so models are not saved too frequently if update % CHECKPOINT_FREQUENCY == 0 : torch . save ( agent . state_dict (), f \" { wandb . run . dir } /agent.pt\" ) wandb . save ( f \" { wandb . run . dir } /agent.pt\" , policy = \"now\" ) To resume training, note the ID of the experiment is 21421tda as in the URL https://wandb.ai/costa-huang/cleanRL/runs/21421tda , so we need to pass in the ID via environment variable to trigger the resume mode of W&B: WANDB_RUN_ID=21421tda WANDB_RESUME=must python ppo_gridnet.py --prod-mode --capture-video","title":"Resume Training"},{"location":"advanced/resume-training/#resume-training","text":"A common question we get asked is how to set up model checkpoints to continue training. In this document, we take this PPO example to explain that question.","title":"Resume Training"},{"location":"advanced/resume-training/#save-model-checkpoints","text":"The first step is to save models periodically. By default, we save the model to wandb . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 num_updates = args . total_timesteps // args . batch_size CHECKPOINT_FREQUENCY = 50 starting_update = 1 for update in range ( starting_update , num_updates + 1 ): # ... do rollouts and train models if args . track : # make sure to tune `CHECKPOINT_FREQUENCY` # so models are not saved too frequently if update % CHECKPOINT_FREQUENCY == 0 : torch . save ( agent . state_dict (), f \" { wandb . run . dir } /agent.pt\" ) wandb . save ( f \" { wandb . run . dir } /agent.pt\" , policy = \"now\" ) Then we could run the following to train our agents python ppo_gridnet.py --prod-mode --capture-video If the training was terminated early, we can still see the last updated model agent.pt in W&B like in this URL https://wandb.ai/costa-huang/cleanRL/runs/21421tda/files or as follows","title":"Save model checkpoints"},{"location":"advanced/resume-training/#resume-training_1","text":"The second step is to automatically download the agent.pt from the URL above and resume training as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 num_updates = args . total_timesteps // args . batch_size CHECKPOINT_FREQUENCY = 50 starting_update = 1 if args . track and wandb . run . resumed : starting_update = run . summary . get ( \"charts/update\" ) + 1 global_step = starting_update * args . batch_size api = wandb . Api () run = api . run ( f \" { run . entity } / { run . project } / { run . id } \" ) model = run . file ( \"agent.pt\" ) model . download ( f \"models/ { experiment_name } /\" ) agent . load_state_dict ( torch . load ( f \"models/ { experiment_name } /agent.pt\" , map_location = device )) agent . eval () print ( f \"resumed at update { starting_update } \" ) for update in range ( starting_update , num_updates + 1 ): # ... do rollouts and train models if args . track : # make sure to tune `CHECKPOINT_FREQUENCY` # so models are not saved too frequently if update % CHECKPOINT_FREQUENCY == 0 : torch . save ( agent . state_dict (), f \" { wandb . run . dir } /agent.pt\" ) wandb . save ( f \" { wandb . run . dir } /agent.pt\" , policy = \"now\" ) To resume training, note the ID of the experiment is 21421tda as in the URL https://wandb.ai/costa-huang/cleanRL/runs/21421tda , so we need to pass in the ID via environment variable to trigger the resume mode of W&B: WANDB_RUN_ID=21421tda WANDB_RESUME=must python ppo_gridnet.py --prod-mode --capture-video","title":"Resume training"},{"location":"cloud/installation/","text":"Installation The rough idea behind the cloud integration is to package our code into a docker container and use AWS Batch to run thousands of experiments concurrently. Prerequisites Terraform (see installation tutorial here ) We use Terraform to define our infrastructure with AWS Batch, which you can spin up as follows # assuming you are at the root of the CleanRL project poetry install -E cloud cd cloud python -m awscli configure terraform init export AWS_DEFAULT_REGION = $( aws configure get region --profile default ) terraform apply Note Don't worry about the cost of spining up these AWS Batch compute environments and job queues. They are completely free and you are only charged when you submit experiments. Then your AWS Batch console should look like Clean Up Uninstalling/Deleting the infrastructure is pretty straightforward: export AWS_DEFAULT_REGION=$(aws configure get region --profile default) terraform destroy","title":"Installation"},{"location":"cloud/installation/#installation","text":"The rough idea behind the cloud integration is to package our code into a docker container and use AWS Batch to run thousands of experiments concurrently.","title":"Installation"},{"location":"cloud/installation/#prerequisites","text":"Terraform (see installation tutorial here ) We use Terraform to define our infrastructure with AWS Batch, which you can spin up as follows # assuming you are at the root of the CleanRL project poetry install -E cloud cd cloud python -m awscli configure terraform init export AWS_DEFAULT_REGION = $( aws configure get region --profile default ) terraform apply Note Don't worry about the cost of spining up these AWS Batch compute environments and job queues. They are completely free and you are only charged when you submit experiments. Then your AWS Batch console should look like","title":"Prerequisites"},{"location":"cloud/installation/#clean-up","text":"Uninstalling/Deleting the infrastructure is pretty straightforward: export AWS_DEFAULT_REGION=$(aws configure get region --profile default) terraform destroy","title":"Clean Up"},{"location":"cloud/submit-experiments/","text":"Submit Experiments Inspection Dry run to inspect the generated docker command poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --num-seed 1 The generated docker command should look like docker run -d --cpuset-cpus=\"0\" -e WANDB_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx vwxyzjn/cleanrl:latest /bin/bash -c \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video --seed 1\" Run on AWS Submit a job using AWS's compute-optimized spot instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --job-queue c5a-large-spot \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-memory 2000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's accelerated-computing spot instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 --track --capture-video\" \\ --job-queue g4dn-xlarge-spot \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-gpu 1 \\ --num-memory 4000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's compute-optimized on-demand instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --job-queue c5a-large \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-memory 2000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's accelerated-computing on-demand instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 --track --capture-video\" \\ --job-queue g4dn-xlarge \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-gpu 1 \\ --num-memory 4000 \\ --num-hours 48.0 \\ --provider aws Then you should see: Customize the Docker Container Set up docker's buildx and login in to your preferred registry. docker buildx create --use docker login Then you could build a container using the --build flag based on the Dockerfile in the current directory. Also, --push will auto-push to the docker registry. poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --build --push To build a multi-arch image using --archs linux/arm64,linux/amd64 : poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --archs linux/arm64,linux/amd64 --build --push Note Building an multi-arch image is quite slow but will allow you to use ARM instances such as m6gd.medium that is 20-70% cheaper than X86 instances. However, note there is no cloud providers that give ARM instances with Nvidia's GPU (to my knowledge), so this effort might not be worth it. If you still wants to pursue multi-arch, you can speed things up by using a native ARM server and connect it to your buildx instance: docker -H ssh://costa@gpu info docker buildx create --name remote --use docker buildx create --name remote --append ssh://costa@gpu docker buildx inspect --bootstrap python -m cleanrl_utils.submit_exp -b --archs linux/arm64,linux/amd64","title":"Submit Experiments"},{"location":"cloud/submit-experiments/#submit-experiments","text":"","title":"Submit Experiments"},{"location":"cloud/submit-experiments/#inspection","text":"Dry run to inspect the generated docker command poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --num-seed 1 The generated docker command should look like docker run -d --cpuset-cpus=\"0\" -e WANDB_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx vwxyzjn/cleanrl:latest /bin/bash -c \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video --seed 1\"","title":"Inspection"},{"location":"cloud/submit-experiments/#run-on-aws","text":"Submit a job using AWS's compute-optimized spot instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --job-queue c5a-large-spot \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-memory 2000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's accelerated-computing spot instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 --track --capture-video\" \\ --job-queue g4dn-xlarge-spot \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-gpu 1 \\ --num-memory 4000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's compute-optimized on-demand instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --job-queue c5a-large \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-memory 2000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's accelerated-computing on-demand instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 --track --capture-video\" \\ --job-queue g4dn-xlarge \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-gpu 1 \\ --num-memory 4000 \\ --num-hours 48.0 \\ --provider aws Then you should see:","title":"Run on AWS"},{"location":"cloud/submit-experiments/#customize-the-docker-container","text":"Set up docker's buildx and login in to your preferred registry. docker buildx create --use docker login Then you could build a container using the --build flag based on the Dockerfile in the current directory. Also, --push will auto-push to the docker registry. poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --build --push To build a multi-arch image using --archs linux/arm64,linux/amd64 : poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --archs linux/arm64,linux/amd64 --build --push Note Building an multi-arch image is quite slow but will allow you to use ARM instances such as m6gd.medium that is 20-70% cheaper than X86 instances. However, note there is no cloud providers that give ARM instances with Nvidia's GPU (to my knowledge), so this effort might not be worth it. If you still wants to pursue multi-arch, you can speed things up by using a native ARM server and connect it to your buildx instance: docker -H ssh://costa@gpu info docker buildx create --name remote --use docker buildx create --name remote --append ssh://costa@gpu docker buildx inspect --bootstrap python -m cleanrl_utils.submit_exp -b --archs linux/arm64,linux/amd64","title":"Customize the Docker Container"},{"location":"get-started/basic-usage/","text":"Basic Usage Two Ways to Run After the dependencies have been installed, there are two ways to run the CleanRL script under the poetry virtual environments. Using poetry run : poetry run python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 Using poetry shell : We first activate the virtual environment by using poetry shell Then, run any desired CleanRL script Attention: Each step must be executed separately! poetry shell python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 Note We recommend poetry shell workflow for development. When the shell is activeated, you should be seeing a prefix like (cleanrl-iXg02GqF-py3.9) in your shell's prompt, which is the name of the poetry's virtual environment. We will assume to run other commands (e.g. tensorboard ) in the documentation within the poetry's shell. Visualize Training Metrics By default, the CleanRL scripts record all the training metrics via Tensorboard into the runs folder. So, after running the training script above, feel free to run tensorboard --logdir runs Visualize the Agent's Gameplay Videos CleanRL helps record the agent's gameplay videos with a --capture-video flag, which will save the videos in the videos/{$run_name} folder. 1 2 3 4 5 python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 \\ --capture-video Get Documentation You can directly obtained the documentation by using the --help flag. python cleanrl/ppo.py --help usage: ppo.py [ -h ] [ --exp-name EXP_NAME ] [ --env-id ENV_ID ] [ --learning-rate LEARNING_RATE ] [ --seed SEED ] [ --total-timesteps TOTAL_TIMESTEPS ] [ --torch-deterministic [ TORCH_DETERMINISTIC ]] [ --cuda [ CUDA ]] [ --track [ TRACK ]] [ --wandb-project-name WANDB_PROJECT_NAME ] [ --wandb-entity WANDB_ENTITY ] [ --capture-video [ CAPTURE_VIDEO ]] [ --num-envs NUM_ENVS ] [ --num-steps NUM_STEPS ] [ --anneal-lr [ ANNEAL_LR ]] [ --gae [ GAE ]] [ --gamma GAMMA ] [ --gae-lambda GAE_LAMBDA ] [ --num-minibatches NUM_MINIBATCHES ] [ --update-epochs UPDATE_EPOCHS ] [ --norm-adv [ NORM_ADV ]] [ --clip-coef CLIP_COEF ] [ --clip-vloss [ CLIP_VLOSS ]] [ --ent-coef ENT_COEF ] [ --vf-coef VF_COEF ] [ --max-grad-norm MAX_GRAD_NORM ] [ --target-kl TARGET_KL ] optional arguments: -h, --help show this help message and exit --exp-name EXP_NAME the name of this experiment --env-id ENV_ID the id of the environment --learning-rate LEARNING_RATE the learning rate of the optimizer --seed SEED seed of the experiment --total-timesteps TOTAL_TIMESTEPS total timesteps of the experiments --torch-deterministic [ TORCH_DETERMINISTIC ] if toggled, ` torch.backends.cudnn.deterministic = False ` --cuda [ CUDA ] if toggled, cuda will be enabled by default --track [ TRACK ] if toggled, this experiment will be tracked with Weights and Biases --wandb-project-name WANDB_PROJECT_NAME the wandb 's project name --wandb-entity WANDB_ENTITY the entity (team) of wandb' s project --capture-video [ CAPTURE_VIDEO ] weather to capture videos of the agent performances ( check out ` videos ` folder ) --num-envs NUM_ENVS the number of parallel game environments --num-steps NUM_STEPS the number of steps to run in each environment per policy rollout --anneal-lr [ ANNEAL_LR ] Toggle learning rate annealing for policy and value networks --gae [ GAE ] Use GAE for advantage computation --gamma GAMMA the discount factor gamma --gae-lambda GAE_LAMBDA the lambda for the general advantage estimation --num-minibatches NUM_MINIBATCHES the number of mini-batches --update-epochs UPDATE_EPOCHS the K epochs to update the policy --norm-adv [ NORM_ADV ] Toggles advantages normalization --clip-coef CLIP_COEF the surrogate clipping coefficient --clip-vloss [ CLIP_VLOSS ] Toggles whether or not to use a clipped loss for the value function , as per the paper. --ent-coef ENT_COEF coefficient of the entropy --vf-coef VF_COEF coefficient of the value function --max-grad-norm MAX_GRAD_NORM the maximum norm for the gradient clipping --target-kl TARGET_KL the target KL divergence threshold","title":"Basic Usage"},{"location":"get-started/basic-usage/#basic-usage","text":"","title":"Basic Usage"},{"location":"get-started/basic-usage/#two-ways-to-run","text":"After the dependencies have been installed, there are two ways to run the CleanRL script under the poetry virtual environments. Using poetry run : poetry run python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 Using poetry shell : We first activate the virtual environment by using poetry shell Then, run any desired CleanRL script Attention: Each step must be executed separately! poetry shell python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 Note We recommend poetry shell workflow for development. When the shell is activeated, you should be seeing a prefix like (cleanrl-iXg02GqF-py3.9) in your shell's prompt, which is the name of the poetry's virtual environment. We will assume to run other commands (e.g. tensorboard ) in the documentation within the poetry's shell.","title":"Two Ways to Run"},{"location":"get-started/basic-usage/#visualize-training-metrics","text":"By default, the CleanRL scripts record all the training metrics via Tensorboard into the runs folder. So, after running the training script above, feel free to run tensorboard --logdir runs","title":"Visualize Training Metrics"},{"location":"get-started/basic-usage/#visualize-the-agents-gameplay-videos","text":"CleanRL helps record the agent's gameplay videos with a --capture-video flag, which will save the videos in the videos/{$run_name} folder. 1 2 3 4 5 python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 \\ --capture-video","title":"Visualize the Agent's Gameplay Videos"},{"location":"get-started/basic-usage/#get-documentation","text":"You can directly obtained the documentation by using the --help flag. python cleanrl/ppo.py --help usage: ppo.py [ -h ] [ --exp-name EXP_NAME ] [ --env-id ENV_ID ] [ --learning-rate LEARNING_RATE ] [ --seed SEED ] [ --total-timesteps TOTAL_TIMESTEPS ] [ --torch-deterministic [ TORCH_DETERMINISTIC ]] [ --cuda [ CUDA ]] [ --track [ TRACK ]] [ --wandb-project-name WANDB_PROJECT_NAME ] [ --wandb-entity WANDB_ENTITY ] [ --capture-video [ CAPTURE_VIDEO ]] [ --num-envs NUM_ENVS ] [ --num-steps NUM_STEPS ] [ --anneal-lr [ ANNEAL_LR ]] [ --gae [ GAE ]] [ --gamma GAMMA ] [ --gae-lambda GAE_LAMBDA ] [ --num-minibatches NUM_MINIBATCHES ] [ --update-epochs UPDATE_EPOCHS ] [ --norm-adv [ NORM_ADV ]] [ --clip-coef CLIP_COEF ] [ --clip-vloss [ CLIP_VLOSS ]] [ --ent-coef ENT_COEF ] [ --vf-coef VF_COEF ] [ --max-grad-norm MAX_GRAD_NORM ] [ --target-kl TARGET_KL ] optional arguments: -h, --help show this help message and exit --exp-name EXP_NAME the name of this experiment --env-id ENV_ID the id of the environment --learning-rate LEARNING_RATE the learning rate of the optimizer --seed SEED seed of the experiment --total-timesteps TOTAL_TIMESTEPS total timesteps of the experiments --torch-deterministic [ TORCH_DETERMINISTIC ] if toggled, ` torch.backends.cudnn.deterministic = False ` --cuda [ CUDA ] if toggled, cuda will be enabled by default --track [ TRACK ] if toggled, this experiment will be tracked with Weights and Biases --wandb-project-name WANDB_PROJECT_NAME the wandb 's project name --wandb-entity WANDB_ENTITY the entity (team) of wandb' s project --capture-video [ CAPTURE_VIDEO ] weather to capture videos of the agent performances ( check out ` videos ` folder ) --num-envs NUM_ENVS the number of parallel game environments --num-steps NUM_STEPS the number of steps to run in each environment per policy rollout --anneal-lr [ ANNEAL_LR ] Toggle learning rate annealing for policy and value networks --gae [ GAE ] Use GAE for advantage computation --gamma GAMMA the discount factor gamma --gae-lambda GAE_LAMBDA the lambda for the general advantage estimation --num-minibatches NUM_MINIBATCHES the number of mini-batches --update-epochs UPDATE_EPOCHS the K epochs to update the policy --norm-adv [ NORM_ADV ] Toggles advantages normalization --clip-coef CLIP_COEF the surrogate clipping coefficient --clip-vloss [ CLIP_VLOSS ] Toggles whether or not to use a clipped loss for the value function , as per the paper. --ent-coef ENT_COEF coefficient of the entropy --vf-coef VF_COEF coefficient of the value function --max-grad-norm MAX_GRAD_NORM the maximum norm for the gradient clipping --target-kl TARGET_KL the target KL divergence threshold","title":"Get Documentation"},{"location":"get-started/benchmark-utility/","text":"Benchmark Utility CleanRL comes with a utility module cleanrl_utils.benchmark to help schedule and run benchmark experiments on your local machine. Usage Try running python -m cleanrl_utils.benchmark --help to get the help text. python -m cleanrl_utils.benchmark --help usage: benchmark.py [ -h ] [ --env-ids ENV_IDS [ ENV_IDS ... ]] [ --command COMMAND ] [ --num-seeds NUM_SEEDS ] [ --workers WORKERS ] optional arguments: -h, --help show this help message and exit --env-ids ENV_IDS [ ENV_IDS ... ] the ids of the environment to benchmark --command COMMAND the command to run --num-seeds NUM_SEEDS the number of random seeds --workers WORKERS the number of eval workers to run benchmark experimenets ( skips evaluation when set to 0 ) Examples The following example demonstrates how to run classic control benchmark experiments. OMP_NUM_THREADS = 1 xvfb-run -a python -m cleanrl_utils.benchmark \\ --env-ids CartPole-v1 Acrobot-v1 MountainCar-v0 \\ --command \"poetry run python cleanrl/ppo.py --cuda False --track --capture-video\" \\ --num-seeds 3 \\ --workers 5 What just happened here? In principle the helps run the following commands in 5 subprocesses: poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id CartPole-v1 --seed 1 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id Acrobot-v1 --seed 1 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id MountainCar-v0 --seed 1 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id CartPole-v1 --seed 2 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id Acrobot-v1 --seed 2 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id MountainCar-v0 --seed 2 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id CartPole-v1 --seed 3 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id Acrobot-v1 --seed 3 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id MountainCar-v0 --seed 3 More specifically: --env-ids CartPole-v1 Acrobot-v1 MountainCar-v0 specifies that running experiments against these three environments --command \"poetry run python cleanrl/ppo.py --cuda False --track --capture-video\" suggests running ppo.py with these settings: turn off GPU usage via --cuda False : because ppo.py has such as small neural network it often runs faster on CPU only track the experiments via --track render the agent gameplay videos via --capture-video ; these videos algo get saved to the tracked experiments xvfb-run -a virtualizes a display for video recording, enabling these commands on a headless linux system --num-seeds 3 suggests running the the command with 3 random seeds for each env-id --workers 5 suggests at maximum using 5 subprocesses to run the experiments OMP_NUM_THREADS=1 suggests torch to use only 1 thread for each subprocesses; this way we don't have processes fighting each other. Note that when you run with high-throughput environments such as envpool or procgen , it's recommended to set --workers 1 to maximuize SPS (steps per second), such as xvfb-run -a python -m cleanrl_utils.benchmark \\ --env-ids Pong-v5 BeamRider-v5 Breakout-v5 \\ --command \"poetry run python cleanrl/ppo_atari_envpool.py --track --capture-video\" \\ --num-seeds 3 \\ --workers 1 For more example usage, see https://github.com/vwxyzjn/cleanrl/blob/master/benchmark/ppo.sh","title":"Benchmark Utility"},{"location":"get-started/benchmark-utility/#benchmark-utility","text":"CleanRL comes with a utility module cleanrl_utils.benchmark to help schedule and run benchmark experiments on your local machine.","title":"Benchmark Utility"},{"location":"get-started/benchmark-utility/#usage","text":"Try running python -m cleanrl_utils.benchmark --help to get the help text. python -m cleanrl_utils.benchmark --help usage: benchmark.py [ -h ] [ --env-ids ENV_IDS [ ENV_IDS ... ]] [ --command COMMAND ] [ --num-seeds NUM_SEEDS ] [ --workers WORKERS ] optional arguments: -h, --help show this help message and exit --env-ids ENV_IDS [ ENV_IDS ... ] the ids of the environment to benchmark --command COMMAND the command to run --num-seeds NUM_SEEDS the number of random seeds --workers WORKERS the number of eval workers to run benchmark experimenets ( skips evaluation when set to 0 )","title":"Usage"},{"location":"get-started/benchmark-utility/#examples","text":"The following example demonstrates how to run classic control benchmark experiments. OMP_NUM_THREADS = 1 xvfb-run -a python -m cleanrl_utils.benchmark \\ --env-ids CartPole-v1 Acrobot-v1 MountainCar-v0 \\ --command \"poetry run python cleanrl/ppo.py --cuda False --track --capture-video\" \\ --num-seeds 3 \\ --workers 5 What just happened here? In principle the helps run the following commands in 5 subprocesses: poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id CartPole-v1 --seed 1 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id Acrobot-v1 --seed 1 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id MountainCar-v0 --seed 1 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id CartPole-v1 --seed 2 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id Acrobot-v1 --seed 2 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id MountainCar-v0 --seed 2 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id CartPole-v1 --seed 3 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id Acrobot-v1 --seed 3 poetry run python cleanrl/ppo.py --cuda False --track --capture-video --env-id MountainCar-v0 --seed 3 More specifically: --env-ids CartPole-v1 Acrobot-v1 MountainCar-v0 specifies that running experiments against these three environments --command \"poetry run python cleanrl/ppo.py --cuda False --track --capture-video\" suggests running ppo.py with these settings: turn off GPU usage via --cuda False : because ppo.py has such as small neural network it often runs faster on CPU only track the experiments via --track render the agent gameplay videos via --capture-video ; these videos algo get saved to the tracked experiments xvfb-run -a virtualizes a display for video recording, enabling these commands on a headless linux system --num-seeds 3 suggests running the the command with 3 random seeds for each env-id --workers 5 suggests at maximum using 5 subprocesses to run the experiments OMP_NUM_THREADS=1 suggests torch to use only 1 thread for each subprocesses; this way we don't have processes fighting each other. Note that when you run with high-throughput environments such as envpool or procgen , it's recommended to set --workers 1 to maximuize SPS (steps per second), such as xvfb-run -a python -m cleanrl_utils.benchmark \\ --env-ids Pong-v5 BeamRider-v5 Breakout-v5 \\ --command \"poetry run python cleanrl/ppo_atari_envpool.py --track --capture-video\" \\ --num-seeds 3 \\ --workers 1 For more example usage, see https://github.com/vwxyzjn/cleanrl/blob/master/benchmark/ppo.sh","title":"Examples"},{"location":"get-started/examples/","text":"Examples Atari poetry shell poetry install -E atari python cleanrl/dqn_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/c51_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 # NEW: 3-4x side-effects free speed up with envpool's atari (only available to linux) poetry install -E envpool python cleanrl/ppo_atari_envpool.py --env-id BreakoutNoFrameskip-v4 # Learn Pong-v5 in ~5-10 mins # Side effects such as lower sample efficiency might occur poetry run python ppo_atari_envpool.py --clip-coef=0.2 --num-envs=16 --num-minibatches=8 --num-steps=128 --update-epochs=3 Demo You can also run training scripts in other games, such as: Classic Control poetry shell python cleanrl/dqn.py --env-id CartPole-v1 python cleanrl/ppo.py --env-id CartPole-v1 python cleanrl/c51.py --env-id CartPole-v1 PyBullet poetry shell poetry install -E pybullet python cleanrl/td3_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 python cleanrl/ddpg_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 python cleanrl/sac_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 Procgen poetry shell poetry install -E procgen python cleanrl/ppo_procgen.py --env-id starpilot python cleanrl/ppg_procgen.py --env-id starpilot PPO + LSTM poetry shell poetry install -E atari python cleanrl/ppo_atari_lstm.py --env-id BreakoutNoFrameskip-v4 python cleanrl/ppo_memory_env_lstm.py","title":"Examples"},{"location":"get-started/examples/#examples","text":"","title":"Examples"},{"location":"get-started/examples/#atari","text":"poetry shell poetry install -E atari python cleanrl/dqn_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/c51_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 # NEW: 3-4x side-effects free speed up with envpool's atari (only available to linux) poetry install -E envpool python cleanrl/ppo_atari_envpool.py --env-id BreakoutNoFrameskip-v4 # Learn Pong-v5 in ~5-10 mins # Side effects such as lower sample efficiency might occur poetry run python ppo_atari_envpool.py --clip-coef=0.2 --num-envs=16 --num-minibatches=8 --num-steps=128 --update-epochs=3","title":"Atari"},{"location":"get-started/examples/#demo","text":"You can also run training scripts in other games, such as:","title":"Demo"},{"location":"get-started/examples/#classic-control","text":"poetry shell python cleanrl/dqn.py --env-id CartPole-v1 python cleanrl/ppo.py --env-id CartPole-v1 python cleanrl/c51.py --env-id CartPole-v1","title":"Classic Control"},{"location":"get-started/examples/#pybullet","text":"poetry shell poetry install -E pybullet python cleanrl/td3_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 python cleanrl/ddpg_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 python cleanrl/sac_continuous_action.py --env-id MinitaurBulletDuckEnv-v0","title":"PyBullet"},{"location":"get-started/examples/#procgen","text":"poetry shell poetry install -E procgen python cleanrl/ppo_procgen.py --env-id starpilot python cleanrl/ppg_procgen.py --env-id starpilot","title":"Procgen"},{"location":"get-started/examples/#ppo-lstm","text":"poetry shell poetry install -E atari python cleanrl/ppo_atari_lstm.py --env-id BreakoutNoFrameskip-v4 python cleanrl/ppo_memory_env_lstm.py","title":"PPO + LSTM"},{"location":"get-started/experiment-tracking/","text":"Experiment tracking To use experiment tracking with wandb, run with the --track flag, which will also upload the videos recorded by the --capture-video flag. poetry shell wandb login # only required for the first time python cleanrl/ppo.py --track --capture-video The console will output the url for the tracked experiment like the following wandb: View project at https://wandb.ai/costa-huang/cleanRL wandb: View run at https://wandb.ai/costa-huang/cleanRL/runs/10dwbgeh When you open the URL, it's going to look like the following page:","title":"Experiment tracking"},{"location":"get-started/experiment-tracking/#experiment-tracking","text":"To use experiment tracking with wandb, run with the --track flag, which will also upload the videos recorded by the --capture-video flag. poetry shell wandb login # only required for the first time python cleanrl/ppo.py --track --capture-video The console will output the url for the tracked experiment like the following wandb: View project at https://wandb.ai/costa-huang/cleanRL wandb: View run at https://wandb.ai/costa-huang/cleanRL/runs/10dwbgeh When you open the URL, it's going to look like the following page:","title":"Experiment tracking"},{"location":"get-started/installation/","text":"Installation Prerequisites Python 3.8+ Poetry Simply run the following command for a quick start git clone https://github.com/vwxyzjn/cleanrl.git && cd cleanrl poetry install Working with PyPI mirrors Users in some countries (e.g., China) can usually speed up package installation via faster PyPI mirrors. If this helps you, try appending the following lines to the pyproject.toml at the root of this repository and run poetry install [[tool.poetry.source]] name = \"douban\" url = \"https://pypi.doubanio.com/simple/\" default = true Optional Dependencies CleanRL makes it easy to install optional dependencies for common RL environments and various development utilities. These optional dependencies are defined at pyproject.toml as shown below: atari = [ \"ale-py\" , \"AutoROM\" , \"stable-baselines3\" ] pybullet = [ \"pybullet\" ] procgen = [ \"procgen\" , \"stable-baselines3\" ] pettingzoo = [ \"pettingzoo\" , \"stable-baselines3\" , \"pygame\" , \"pymunk\" ] plot = [ \"pandas\" , \"seaborn\" ] cloud = [ \"boto3\" , \"awscli\" ] docs = [ \"mkdocs-material\" ] spyder = [ \"spyder\" ] You can install them using the following command poetry install -E atari poetry install -E pybullet poetry install -E mujoco poetry install -E procgen poetry install -E envpool poetry install -E pettingzoo Install via pip While we recommend using poetry to manage environments and dependencies, the traditional requirements.txt are available: pip install -r requirements/requirements.txt pip install -r requirements/requirements-atari.txt pip install -r requirements/requirements-pybullet.txt pip install -r requirements/requirements-mujoco.txt pip install -r requirements/requirements-procgen.txt pip install -r requirements/requirements-envpool.txt pip install -r requirements/requirements-pettingzoo.txt","title":"Installation"},{"location":"get-started/installation/#installation","text":"","title":"Installation"},{"location":"get-started/installation/#prerequisites","text":"Python 3.8+ Poetry Simply run the following command for a quick start git clone https://github.com/vwxyzjn/cleanrl.git && cd cleanrl poetry install Working with PyPI mirrors Users in some countries (e.g., China) can usually speed up package installation via faster PyPI mirrors. If this helps you, try appending the following lines to the pyproject.toml at the root of this repository and run poetry install [[tool.poetry.source]] name = \"douban\" url = \"https://pypi.doubanio.com/simple/\" default = true","title":"Prerequisites"},{"location":"get-started/installation/#optional-dependencies","text":"CleanRL makes it easy to install optional dependencies for common RL environments and various development utilities. These optional dependencies are defined at pyproject.toml as shown below: atari = [ \"ale-py\" , \"AutoROM\" , \"stable-baselines3\" ] pybullet = [ \"pybullet\" ] procgen = [ \"procgen\" , \"stable-baselines3\" ] pettingzoo = [ \"pettingzoo\" , \"stable-baselines3\" , \"pygame\" , \"pymunk\" ] plot = [ \"pandas\" , \"seaborn\" ] cloud = [ \"boto3\" , \"awscli\" ] docs = [ \"mkdocs-material\" ] spyder = [ \"spyder\" ] You can install them using the following command poetry install -E atari poetry install -E pybullet poetry install -E mujoco poetry install -E procgen poetry install -E envpool poetry install -E pettingzoo","title":"Optional Dependencies"},{"location":"get-started/installation/#install-via-pip","text":"While we recommend using poetry to manage environments and dependencies, the traditional requirements.txt are available: pip install -r requirements/requirements.txt pip install -r requirements/requirements-atari.txt pip install -r requirements/requirements-pybullet.txt pip install -r requirements/requirements-mujoco.txt pip install -r requirements/requirements-procgen.txt pip install -r requirements/requirements-envpool.txt pip install -r requirements/requirements-pettingzoo.txt","title":"Install via pip"},{"location":"rl-algorithms/c51/","text":"Categorical DQN (C51) Overview C51 introduces a distributional perspective for DQN: instead of learning a single value for an action, C51 learns to predict a distribution of values for the action. Empirically, C51 demonstrates impressive performance in ALE. Original papers: A Distributional Perspective on Reinforcement Learning Implemented Variants Variants Implemented Description c51_atari.py , docs For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. c51.py , docs For classic control tasks like CartPole-v1 . Below are our single-file implementations of C51: c51_atari.py The c51_atari.py has the following features: For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space Usage poetry install -E atari python cleanrl/c51_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/c51_atari.py --env-id PongNoFrameskip-v4 Explanation of the logged metrics Running python cleanrl/c51_atari.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/loss : the cross entropy loss between the \\(t\\) step state value distribution and the projected \\(t+1\\) step state value distribution losses/q_values : implemented as (old_pmfs * q_network.atoms).sum(1) , which is the sum of the probability of getting returns \\(x\\) ( old_pmfs ) multiplied by \\(x\\) ( q_network.atoms ), averaged over the sample obtained from the replay buffer; useful when gauging if under or over estimation happens Implementation details c51_atari.py is based on (Bellemare et al., 2017) 1 but presents a few implementation differences: (Bellemare et al., 2017) 1 injects stochaticity by doing \"on each frame the environment rejects the agent\u2019s selected action with probability \\(p = 0.25\\) \", but c51_atari.py does not do this c51_atari.py use a self-contained evaluation scheme: c51_atari.py reports the episodic returns obtained throughout training, whereas (Bellemare et al., 2017) 1 is trained with --end-e=0.01 but reported episodic returns using a separate evaluation process with --end-e=0.001 (See \"5.2. State-of-the-Art Results\" on page 7). c51_atari.py rescales the gradient so that the norm of the parameters does not exceed 0.5 like done in PPO ( ppo2/model.py#L102-L108 ). Experiment results PR vwxyzjn/cleanrl#159 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/c51 . Below are the average episodic returns for c51_atari.py . Environment c51_atari.py 10M steps (Bellemare et al., 2017, Figure 14) 1 50M steps (Hessel et al., 2017, Figure 5) 3 BreakoutNoFrameskip-v4 467.00 \u00b1 96.11 748 ~500 at 10M steps, ~600 at 50M steps PongNoFrameskip-v4 19.32 \u00b1 0.92 20.9 ~20 10M steps, ~20 at 50M steps BeamRiderNoFrameskip-v4 9986.96 \u00b1 1953.30 14,074 ~12000 10M steps, ~14000 at 50M steps Note that we save computational time by reducing timesteps from 50M to 10M, but our c51_atari.py scores the same or higher than (Mnih et al., 2015) 1 in 10M steps. Learning curves: Tracked experiments and game play videos: c51.py The c51.py has the following features: Works with the Box observation space of low-level features Works with the Discrete action space Works with envs like CartPole-v1 Usage python cleanrl/c51.py --env-id CartPole-v1 Explanation of the logged metrics See related docs for c51_atari.py . Implementation details The c51.py shares the same implementation details as c51_atari.py except the c51.py runs with different hyperparameters and neural network architecture. Specifically, c51.py uses a simpler neural network as follows: self . network = nn . Sequential ( nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 120 ), nn . ReLU (), nn . Linear ( 120 , 84 ), nn . ReLU (), nn . Linear ( 84 , env . single_action_space . n ), ) c51.py runs with different hyperparameters: python c51.py --total-timesteps 500000 \\ --learning-rate 2 .5e-4 \\ --buffer-size 10000 \\ --gamma 0 .99 \\ --target-network-frequency 500 \\ --max-grad-norm 0 .5 \\ --batch-size 128 \\ --start-e 1 \\ --end-e 0 .05 \\ --exploration-fraction 0 .5 \\ --learning-starts 10000 \\ --train-frequency 10 Experiment results PR vwxyzjn/cleanrl#159 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/c51 . Below are the average episodic returns for c51.py . Environment c51.py CartPole-v1 498.51 \u00b1 1.77 Acrobot-v1 -88.81 \u00b1 8.86 MountainCar-v0 -167.71 \u00b1 26.85 Note that the C51 has no official benchmark on classic control environments, so we did not include a comparison. That said, our c51.py was able to achieve near perfect scores in CartPole-v1 and Acrobot-v1 ; further, it can obtain successful runs in the sparse environment MountainCar-v0 . Learning curves: Tracked experiments and game play videos: Bellemare, M.G., Dabney, W., & Munos, R. (2017). A Distributional Perspective on Reinforcement Learning. ICML. \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 [Proposal] Formal API handling of truncation vs termination. https://github.com/openai/gym/issues/2510 \u21a9 Hessel, M., Modayil, J., Hasselt, H.V., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M.G., & Silver, D. (2018). Rainbow: Combining Improvements in Deep Reinforcement Learning. AAAI. \u21a9","title":"Categorical DQN (C51)"},{"location":"rl-algorithms/c51/#categorical-dqn-c51","text":"","title":"Categorical DQN (C51)"},{"location":"rl-algorithms/c51/#overview","text":"C51 introduces a distributional perspective for DQN: instead of learning a single value for an action, C51 learns to predict a distribution of values for the action. Empirically, C51 demonstrates impressive performance in ALE. Original papers: A Distributional Perspective on Reinforcement Learning","title":"Overview"},{"location":"rl-algorithms/c51/#implemented-variants","text":"Variants Implemented Description c51_atari.py , docs For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. c51.py , docs For classic control tasks like CartPole-v1 . Below are our single-file implementations of C51:","title":"Implemented Variants"},{"location":"rl-algorithms/c51/#c51_ataripy","text":"The c51_atari.py has the following features: For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space","title":"c51_atari.py"},{"location":"rl-algorithms/c51/#usage","text":"poetry install -E atari python cleanrl/c51_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/c51_atari.py --env-id PongNoFrameskip-v4","title":"Usage"},{"location":"rl-algorithms/c51/#explanation-of-the-logged-metrics","text":"Running python cleanrl/c51_atari.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/loss : the cross entropy loss between the \\(t\\) step state value distribution and the projected \\(t+1\\) step state value distribution losses/q_values : implemented as (old_pmfs * q_network.atoms).sum(1) , which is the sum of the probability of getting returns \\(x\\) ( old_pmfs ) multiplied by \\(x\\) ( q_network.atoms ), averaged over the sample obtained from the replay buffer; useful when gauging if under or over estimation happens","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/c51/#implementation-details","text":"c51_atari.py is based on (Bellemare et al., 2017) 1 but presents a few implementation differences: (Bellemare et al., 2017) 1 injects stochaticity by doing \"on each frame the environment rejects the agent\u2019s selected action with probability \\(p = 0.25\\) \", but c51_atari.py does not do this c51_atari.py use a self-contained evaluation scheme: c51_atari.py reports the episodic returns obtained throughout training, whereas (Bellemare et al., 2017) 1 is trained with --end-e=0.01 but reported episodic returns using a separate evaluation process with --end-e=0.001 (See \"5.2. State-of-the-Art Results\" on page 7). c51_atari.py rescales the gradient so that the norm of the parameters does not exceed 0.5 like done in PPO ( ppo2/model.py#L102-L108 ).","title":"Implementation details"},{"location":"rl-algorithms/c51/#experiment-results","text":"PR vwxyzjn/cleanrl#159 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/c51 . Below are the average episodic returns for c51_atari.py . Environment c51_atari.py 10M steps (Bellemare et al., 2017, Figure 14) 1 50M steps (Hessel et al., 2017, Figure 5) 3 BreakoutNoFrameskip-v4 467.00 \u00b1 96.11 748 ~500 at 10M steps, ~600 at 50M steps PongNoFrameskip-v4 19.32 \u00b1 0.92 20.9 ~20 10M steps, ~20 at 50M steps BeamRiderNoFrameskip-v4 9986.96 \u00b1 1953.30 14,074 ~12000 10M steps, ~14000 at 50M steps Note that we save computational time by reducing timesteps from 50M to 10M, but our c51_atari.py scores the same or higher than (Mnih et al., 2015) 1 in 10M steps. Learning curves: Tracked experiments and game play videos:","title":"Experiment results"},{"location":"rl-algorithms/c51/#c51py","text":"The c51.py has the following features: Works with the Box observation space of low-level features Works with the Discrete action space Works with envs like CartPole-v1","title":"c51.py"},{"location":"rl-algorithms/c51/#usage_1","text":"python cleanrl/c51.py --env-id CartPole-v1","title":"Usage"},{"location":"rl-algorithms/c51/#explanation-of-the-logged-metrics_1","text":"See related docs for c51_atari.py .","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/c51/#implementation-details_1","text":"The c51.py shares the same implementation details as c51_atari.py except the c51.py runs with different hyperparameters and neural network architecture. Specifically, c51.py uses a simpler neural network as follows: self . network = nn . Sequential ( nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 120 ), nn . ReLU (), nn . Linear ( 120 , 84 ), nn . ReLU (), nn . Linear ( 84 , env . single_action_space . n ), ) c51.py runs with different hyperparameters: python c51.py --total-timesteps 500000 \\ --learning-rate 2 .5e-4 \\ --buffer-size 10000 \\ --gamma 0 .99 \\ --target-network-frequency 500 \\ --max-grad-norm 0 .5 \\ --batch-size 128 \\ --start-e 1 \\ --end-e 0 .05 \\ --exploration-fraction 0 .5 \\ --learning-starts 10000 \\ --train-frequency 10","title":"Implementation details"},{"location":"rl-algorithms/c51/#experiment-results_1","text":"PR vwxyzjn/cleanrl#159 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/c51 . Below are the average episodic returns for c51.py . Environment c51.py CartPole-v1 498.51 \u00b1 1.77 Acrobot-v1 -88.81 \u00b1 8.86 MountainCar-v0 -167.71 \u00b1 26.85 Note that the C51 has no official benchmark on classic control environments, so we did not include a comparison. That said, our c51.py was able to achieve near perfect scores in CartPole-v1 and Acrobot-v1 ; further, it can obtain successful runs in the sparse environment MountainCar-v0 . Learning curves: Tracked experiments and game play videos: Bellemare, M.G., Dabney, W., & Munos, R. (2017). A Distributional Perspective on Reinforcement Learning. ICML. \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 [Proposal] Formal API handling of truncation vs termination. https://github.com/openai/gym/issues/2510 \u21a9 Hessel, M., Modayil, J., Hasselt, H.V., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M.G., & Silver, D. (2018). Rainbow: Combining Improvements in Deep Reinforcement Learning. AAAI. \u21a9","title":"Experiment results"},{"location":"rl-algorithms/ddpg/","text":"Deep Deterministic Policy Gradient (DDPG) Overview DDPG is a popular DRL algorithm for continuous control. It extends DQN to work with the continuous action space by introducing a deterministic actor that directly outputs continuous actions. DDPG also combines techniques from DQN, such as the replay buffer and target network. Original paper: Continuous control with deep reinforcement learning Reference resources: sfujim/TD3 Deep Deterministic Policy Gradient | Spinning Up in Deep RL Implemented Variants Variants Implemented Description ddpg_continuous_action.py , docs For continuous action space Below is our single-file implementation of DDPG: ddpg_continuous_action.py The ddpg_continuous_action.py has the following features: For continuous action space Works with the Box observation space of low-level features Works with the Box (continuous) action space Usage poetry install poetry install -E pybullet python cleanrl/ddpg_continuous_action.py --help python cleanrl/ddpg_continuous_action.py --env-id HopperBulletEnv-v0 poetry install -E mujoco # only works in Linux python cleanrl/ddpg_continuous_action.py --env-id Hopper-v3 Explanation of the logged metrics Running python cleanrl/ddpg_continuous_action.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/qf1_loss : the mean squared error (MSE) between the Q values at timestep \\(t\\) and the Bellman update target estimated using the reward \\(r_t\\) and the Q values at timestep \\(t+1\\) , thus minimizing the one-step temporal difference. Formally, it can be expressed by the equation below. $$ J(\\theta^{Q}) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\big[ (Q(s, a) - y)^2 \\big], $$ with the Bellman update target \\(y = r + \\gamma \\, Q^{'}(s', a')\\) , where \\(a' \\sim \\mu^{'}(s')\\) , and the replay buffer \\(\\mathcal{D}\\) . losses/actor_loss : implemented as -qf1(data.observations, actor(data.observations)).mean() ; it is the negative average Q values calculated based on the 1) observations and the 2) actions computed by the actor based on these observations. By minimizing actor_loss , the optimizer updates the actors parameter using the following gradient (Lillicrap et al., 2016, Algorithm 1) 1 : \\[ \\nabla_{\\theta^{\\mu}} J \\approx \\frac{1}{N}\\sum_i\\left.\\left.\\nabla_{a} Q\\left(s, a \\mid \\theta^{Q}\\right)\\right|_{s=s_{i}, a=\\mu\\left(s_{i}\\right)} \\nabla_{\\theta^{\\mu}} \\mu\\left(s \\mid \\theta^{\\mu}\\right)\\right|_{s_{i}} \\] losses/qf1_values : implemented as qf1(data.observations, data.actions).view(-1) , it is the average Q values of the sampled data in the replay buffer; useful when gauging if under or over estimation happens. Implementation details Our ddpg_continuous_action.py is based on the OurDDPG.py from sfujim/TD3 , which presents the the following implementation difference from (Lillicrap et al., 2016) 1 : ddpg_continuous_action.py uses a gaussian exploration noise \\(\\mathcal{N}(0, 0.1)\\) , while (Lillicrap et al., 2016) 1 uses Ornstein-Uhlenbeck process with \\(\\theta=0.15\\) and \\(\\sigma=0.2\\) . ddpg_continuous_action.py runs the experiments using the openai/gym MuJoCo environments, while (Lillicrap et al., 2016) 1 uses their proprietary MuJoCo environments. ddpg_continuous_action.py uses the following architecture: class QNetwork ( nn . Module ): def __init__ ( self , env ): super ( QNetwork , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod () + np . prod ( env . single_action_space . shape ), 256 ) self . fc2 = nn . Linear ( 256 , 256 ) self . fc3 = nn . Linear ( 256 , 1 ) def forward ( self , x , a ): x = torch . cat ([ x , a ], 1 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x class Actor ( nn . Module ): def __init__ ( self , env ): super ( Actor , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 256 ) self . fc2 = nn . Linear ( 256 , 256 ) self . fc_mu = nn . Linear ( 256 , np . prod ( env . single_action_space . shape )) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) return torch . tanh ( self . fc_mu ( x )) while (Lillicrap et al., 2016, see Appendix 7 EXPERIMENT DETAILS) 1 uses the following architecture (difference highlighted): class QNetwork ( nn . Module ): def __init__ ( self , env ): super ( QNetwork , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 400 ) self . fc2 = nn . Linear ( 400 + np . prod ( env . single_action_space . shape ), 300 ) self . fc3 = nn . Linear ( 300 , 1 ) def forward ( self , x , a ): x = F . relu ( self . fc1 ( x )) x = torch . cat ([ x , a ], 1 ) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x class Actor ( nn . Module ): def __init__ ( self , env ): super ( Actor , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 400 ) self . fc2 = nn . Linear ( 400 , 300 ) self . fc_mu = nn . Linear ( 300 , np . prod ( env . single_action_space . shape )) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) return torch . tanh ( self . fc_mu ( x )) ddpg_continuous_action.py uses the following learning rates: q_optimizer = optim . Adam ( list ( qf1 . parameters ()), lr = 3e-4 ) actor_optimizer = optim . Adam ( list ( actor . parameters ()), lr = 3e-4 ) while (Lillicrap et al., 2016, see Appendix 7 EXPERIMENT DETAILS) 1 uses the following learning rates: q_optimizer = optim . Adam ( list ( qf1 . parameters ()), lr = 1e-4 ) actor_optimizer = optim . Adam ( list ( actor . parameters ()), lr = 1e-3 ) ddpg_continuous_action.py uses --batch-size=256 --tau=0.005 , while (Lillicrap et al., 2016, see Appendix 7 EXPERIMENT DETAILS) 1 uses --batch-size=64 --tau=0.001 ddpg_continuous_action.py rescales the gradient so that the norm of the parameters does not exceed 0.5 like done in PPO ( ppo2/model.py#L102-L108 ). Experiment results PR vwxyzjn/cleanrl#137 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/ddpg . Below are the average episodic returns for ddpg_continuous_action.py (3 random seeds). To ensure the quality of the implementation, we compared the results against (Fujimoto et al., 2018) 2 . Environment ddpg_continuous_action.py OurDDPG.py (Fujimoto et al., 2018, Table 1) 2 DDPG.py using settings from (Lillicrap et al., 2016) 1 in (Fujimoto et al., 2018, Table 1) 2 HalfCheetah 9260.485 \u00b1 643.088 8577.29 3305.60 Walker2d 1728.72 \u00b1 758.33 3098.11 1843.85 Hopper 1404.44 \u00b1 544.78 1860.02 2020.46 Info Note that ddpg_continuous_action.py uses gym MuJoCo v2 environments while OurDDPG.py (Fujimoto et al., 2018) 2 uses the gym MuJoCo v1 environments. According to the openai/gym#834 , gym MuJoCo v2 environments should be equivalent to the gym MuJoCo v1 environments. Also note the performance of our ddpg_continuous_action.py seems to be worse than the reference implementation on Walker2d and Hopper. This is likely due to openai/gym#938 . We would have a hard time reproducing gym MuJoCo v1 environments because they have been long deprecated. One other thing could cause the performance difference: the original code reported the average episodic return using determinisitc evaluation (i.e., without exploration noise), see sfujim/TD3/main.py#L15-L32 , whereas we reported the episodic return during training and the policy gets updated between environments steps. Learning curves: Tracked experiments and game play videos: Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N.M., Erez, T., Tassa, Y., Silver, D., & Wierstra, D. (2016). Continuous control with deep reinforcement learning. CoRR, abs/1509.02971. https://arxiv.org/abs/1509.02971 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Fujimoto, S., Hoof, H.V., & Meger, D. (2018). Addressing Function Approximation Error in Actor-Critic Methods. ArXiv, abs/1802.09477. https://arxiv.org/abs/1802.09477 \u21a9 \u21a9 \u21a9 \u21a9","title":"Deep Deterministic Policy Gradient (DDPG)"},{"location":"rl-algorithms/ddpg/#deep-deterministic-policy-gradient-ddpg","text":"","title":"Deep Deterministic Policy Gradient (DDPG)"},{"location":"rl-algorithms/ddpg/#overview","text":"DDPG is a popular DRL algorithm for continuous control. It extends DQN to work with the continuous action space by introducing a deterministic actor that directly outputs continuous actions. DDPG also combines techniques from DQN, such as the replay buffer and target network. Original paper: Continuous control with deep reinforcement learning Reference resources: sfujim/TD3 Deep Deterministic Policy Gradient | Spinning Up in Deep RL","title":"Overview"},{"location":"rl-algorithms/ddpg/#implemented-variants","text":"Variants Implemented Description ddpg_continuous_action.py , docs For continuous action space Below is our single-file implementation of DDPG:","title":"Implemented Variants"},{"location":"rl-algorithms/ddpg/#ddpg_continuous_actionpy","text":"The ddpg_continuous_action.py has the following features: For continuous action space Works with the Box observation space of low-level features Works with the Box (continuous) action space","title":"ddpg_continuous_action.py"},{"location":"rl-algorithms/ddpg/#usage","text":"poetry install poetry install -E pybullet python cleanrl/ddpg_continuous_action.py --help python cleanrl/ddpg_continuous_action.py --env-id HopperBulletEnv-v0 poetry install -E mujoco # only works in Linux python cleanrl/ddpg_continuous_action.py --env-id Hopper-v3","title":"Usage"},{"location":"rl-algorithms/ddpg/#explanation-of-the-logged-metrics","text":"Running python cleanrl/ddpg_continuous_action.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/qf1_loss : the mean squared error (MSE) between the Q values at timestep \\(t\\) and the Bellman update target estimated using the reward \\(r_t\\) and the Q values at timestep \\(t+1\\) , thus minimizing the one-step temporal difference. Formally, it can be expressed by the equation below. $$ J(\\theta^{Q}) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\big[ (Q(s, a) - y)^2 \\big], $$ with the Bellman update target \\(y = r + \\gamma \\, Q^{'}(s', a')\\) , where \\(a' \\sim \\mu^{'}(s')\\) , and the replay buffer \\(\\mathcal{D}\\) . losses/actor_loss : implemented as -qf1(data.observations, actor(data.observations)).mean() ; it is the negative average Q values calculated based on the 1) observations and the 2) actions computed by the actor based on these observations. By minimizing actor_loss , the optimizer updates the actors parameter using the following gradient (Lillicrap et al., 2016, Algorithm 1) 1 : \\[ \\nabla_{\\theta^{\\mu}} J \\approx \\frac{1}{N}\\sum_i\\left.\\left.\\nabla_{a} Q\\left(s, a \\mid \\theta^{Q}\\right)\\right|_{s=s_{i}, a=\\mu\\left(s_{i}\\right)} \\nabla_{\\theta^{\\mu}} \\mu\\left(s \\mid \\theta^{\\mu}\\right)\\right|_{s_{i}} \\] losses/qf1_values : implemented as qf1(data.observations, data.actions).view(-1) , it is the average Q values of the sampled data in the replay buffer; useful when gauging if under or over estimation happens.","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/ddpg/#implementation-details","text":"Our ddpg_continuous_action.py is based on the OurDDPG.py from sfujim/TD3 , which presents the the following implementation difference from (Lillicrap et al., 2016) 1 : ddpg_continuous_action.py uses a gaussian exploration noise \\(\\mathcal{N}(0, 0.1)\\) , while (Lillicrap et al., 2016) 1 uses Ornstein-Uhlenbeck process with \\(\\theta=0.15\\) and \\(\\sigma=0.2\\) . ddpg_continuous_action.py runs the experiments using the openai/gym MuJoCo environments, while (Lillicrap et al., 2016) 1 uses their proprietary MuJoCo environments. ddpg_continuous_action.py uses the following architecture: class QNetwork ( nn . Module ): def __init__ ( self , env ): super ( QNetwork , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod () + np . prod ( env . single_action_space . shape ), 256 ) self . fc2 = nn . Linear ( 256 , 256 ) self . fc3 = nn . Linear ( 256 , 1 ) def forward ( self , x , a ): x = torch . cat ([ x , a ], 1 ) x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x class Actor ( nn . Module ): def __init__ ( self , env ): super ( Actor , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 256 ) self . fc2 = nn . Linear ( 256 , 256 ) self . fc_mu = nn . Linear ( 256 , np . prod ( env . single_action_space . shape )) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) return torch . tanh ( self . fc_mu ( x )) while (Lillicrap et al., 2016, see Appendix 7 EXPERIMENT DETAILS) 1 uses the following architecture (difference highlighted): class QNetwork ( nn . Module ): def __init__ ( self , env ): super ( QNetwork , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 400 ) self . fc2 = nn . Linear ( 400 + np . prod ( env . single_action_space . shape ), 300 ) self . fc3 = nn . Linear ( 300 , 1 ) def forward ( self , x , a ): x = F . relu ( self . fc1 ( x )) x = torch . cat ([ x , a ], 1 ) x = F . relu ( self . fc2 ( x )) x = self . fc3 ( x ) return x class Actor ( nn . Module ): def __init__ ( self , env ): super ( Actor , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 400 ) self . fc2 = nn . Linear ( 400 , 300 ) self . fc_mu = nn . Linear ( 300 , np . prod ( env . single_action_space . shape )) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) return torch . tanh ( self . fc_mu ( x )) ddpg_continuous_action.py uses the following learning rates: q_optimizer = optim . Adam ( list ( qf1 . parameters ()), lr = 3e-4 ) actor_optimizer = optim . Adam ( list ( actor . parameters ()), lr = 3e-4 ) while (Lillicrap et al., 2016, see Appendix 7 EXPERIMENT DETAILS) 1 uses the following learning rates: q_optimizer = optim . Adam ( list ( qf1 . parameters ()), lr = 1e-4 ) actor_optimizer = optim . Adam ( list ( actor . parameters ()), lr = 1e-3 ) ddpg_continuous_action.py uses --batch-size=256 --tau=0.005 , while (Lillicrap et al., 2016, see Appendix 7 EXPERIMENT DETAILS) 1 uses --batch-size=64 --tau=0.001 ddpg_continuous_action.py rescales the gradient so that the norm of the parameters does not exceed 0.5 like done in PPO ( ppo2/model.py#L102-L108 ).","title":"Implementation details"},{"location":"rl-algorithms/ddpg/#experiment-results","text":"PR vwxyzjn/cleanrl#137 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/ddpg . Below are the average episodic returns for ddpg_continuous_action.py (3 random seeds). To ensure the quality of the implementation, we compared the results against (Fujimoto et al., 2018) 2 . Environment ddpg_continuous_action.py OurDDPG.py (Fujimoto et al., 2018, Table 1) 2 DDPG.py using settings from (Lillicrap et al., 2016) 1 in (Fujimoto et al., 2018, Table 1) 2 HalfCheetah 9260.485 \u00b1 643.088 8577.29 3305.60 Walker2d 1728.72 \u00b1 758.33 3098.11 1843.85 Hopper 1404.44 \u00b1 544.78 1860.02 2020.46 Info Note that ddpg_continuous_action.py uses gym MuJoCo v2 environments while OurDDPG.py (Fujimoto et al., 2018) 2 uses the gym MuJoCo v1 environments. According to the openai/gym#834 , gym MuJoCo v2 environments should be equivalent to the gym MuJoCo v1 environments. Also note the performance of our ddpg_continuous_action.py seems to be worse than the reference implementation on Walker2d and Hopper. This is likely due to openai/gym#938 . We would have a hard time reproducing gym MuJoCo v1 environments because they have been long deprecated. One other thing could cause the performance difference: the original code reported the average episodic return using determinisitc evaluation (i.e., without exploration noise), see sfujim/TD3/main.py#L15-L32 , whereas we reported the episodic return during training and the policy gets updated between environments steps. Learning curves: Tracked experiments and game play videos: Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N.M., Erez, T., Tassa, Y., Silver, D., & Wierstra, D. (2016). Continuous control with deep reinforcement learning. CoRR, abs/1509.02971. https://arxiv.org/abs/1509.02971 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Fujimoto, S., Hoof, H.V., & Meger, D. (2018). Addressing Function Approximation Error in Actor-Critic Methods. ArXiv, abs/1802.09477. https://arxiv.org/abs/1802.09477 \u21a9 \u21a9 \u21a9 \u21a9","title":"Experiment results"},{"location":"rl-algorithms/dqn/","text":"Deep Q-Learning (DQN) Overview As an extension of the Q-learning, DQN's main technical contribution is the use of replay buffer and target network, both of which would help improve the stability of the algorithm. Original papers: Human-level control through deep reinforcement learning Implemented Variants Variants Implemented Description dqn_atari.py , docs For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. dqn.py , docs For classic control tasks like CartPole-v1 . Below are our single-file implementations of DQN: dqn_atari.py The dqn_atari.py has the following features: For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space Usage poetry install -E atari python cleanrl/dqn_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/dqn_atari.py --env-id PongNoFrameskip-v4 Explanation of the logged metrics Running python cleanrl/dqn_atari.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/td_loss : the mean squared error (MSE) between the Q values at timestep \\(t\\) and the Bellman update target estimated using the reward \\(r_t\\) and the Q values at timestep \\(t+1\\) , thus minimizing the one-step temporal difference. Formally, it can be expressed by the equation below. $$ J(\\theta^{Q}) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\big[ (Q(s, a) - y)^2 \\big], $$ with the Bellman update target is \\(y = r + \\gamma \\, Q^{'}(s', a')\\) and the replay buffer is \\(\\mathcal{D}\\) . losses/q_values : implemented as qf1(data.observations, data.actions).view(-1) , it is the average Q values of the sampled data in the replay buffer; useful when gauging if under or over estimation happens. Implementation details dqn_atari.py is based on (Mnih et al., 2015) 1 but presents a few implementation differences: dqn_atari.py use slightly different hyperparameters. Specifically, dqn_atari.py uses the more popular Adam Optimizer with the --learning-rate=1e-4 as follows: optim . Adam ( q_network . parameters (), lr = 1e-4 ) whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses the RMSProp optimizer with --learning-rate=2.5e-4 , gradient momentum 0.95 , squared gradient momentum 0.95 , and min squared gradient 0.01 as follows: optim . RMSprop ( q_network . parameters (), lr = 2.5e-4 , momentum = 0.95 , # ... PyTorch's RMSprop does not directly support # squared gradient momentum and min squared gradient # so we are not sure what to put here. ) dqn_atari.py uses --learning-starts=80000 whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --learning-starts=50000 . dqn_atari.py uses --target-network-frequency=1000 whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --learning-starts=10000 . dqn_atari.py uses --total-timesteps=10000000 (i.e., 10M timesteps = 40M frames because of frame-skipping) whereas (Mnih et al., 2015) 1 uses --total-timesteps=50000000 (i.e., 50M timesteps = 200M frames) (See \"Training details\" under \"METHODS\" on page 6 and the related source code run_gpu#L32 , dqn/train_agent.lua#L81-L82 , and dqn/train_agent.lua#L165-L169 ). dqn_atari.py uses --end-e=0.01 (the final exploration epsilon) whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --end-e=0.1 . dqn_atari.py uses --exploration-fraction=0.1 whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --exploration-fraction=0.02 (all corresponds to 250000 steps or 1M frames being the frame that epsilon is annealed to --end-e=0.1 ). dqn_atari.py handles truncation and termination properly like (Mnih et al., 2015) 1 by using SB3's replay buffer's handle_timeout_termination=True . dqn_atari.py use a self-contained evaluation scheme: dqn_atari.py reports the episodic returns obtained throughout training, whereas (Mnih et al., 2015) 1 is trained with --end-e=0.1 but reported episodic returns using a separate evaluation process with --end-e=0.01 (See \"Evaluation procedure\" under \"METHODS\" on page 6). dqn_atari.py rescales the gradient so that the norm of the parameters does not exceed 0.5 like done in PPO ( ppo2/model.py#L102-L108 ). Experiment results PR vwxyzjn/cleanrl#124 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/dqn . Below are the average episodic returns for dqn_atari.py . Environment dqn_atari.py 10M steps (Mnih et al., 2015) 1 50M steps (Hessel et al., 2017, Figure 5) 3 BreakoutNoFrameskip-v4 337.64 \u00b1 69.47 401.2 \u00b1 26.9 ~230 at 10M steps, ~300 at 50M steps PongNoFrameskip-v4 20.293 \u00b1 0.37 18.9 \u00b1 1.3 ~20 10M steps, ~20 at 50M steps BeamRiderNoFrameskip-v4 6207.41 \u00b1 1019.96 6846 \u00b1 1619 ~6000 10M steps, ~7000 at 50M steps Note that we save computational time by reducing timesteps from 50M to 10M, but our dqn_atari.py scores the same or higher than (Mnih et al., 2015) 1 in 10M steps. Learning curves: Tracked experiments and game play videos: dqn.py The dqn.py has the following features: Works with the Box observation space of low-level features Works with the Discrete action space Works with envs like CartPole-v1 Usage python cleanrl/dqn.py --env-id CartPole-v1 Explanation of the logged metrics See related docs for dqn_atari.py . Implementation details The dqn.py shares the same implementation details as dqn_atari.py except the dqn.py runs with different hyperparameters and neural network architecture. Specifically, dqn.py uses a simpler neural network as follows: self . network = nn . Sequential ( nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 120 ), nn . ReLU (), nn . Linear ( 120 , 84 ), nn . ReLU (), nn . Linear ( 84 , env . single_action_space . n ), ) dqn.py runs with different hyperparameters: python dqn.py --total-timesteps 500000 \\ --learning-rate 2 .5e-4 \\ --buffer-size 10000 \\ --gamma 0 .99 \\ --target-network-frequency 500 \\ --max-grad-norm 0 .5 \\ --batch-size 128 \\ --start-e 1 \\ --end-e 0 .05 \\ --exploration-fraction 0 .5 \\ --learning-starts 10000 \\ --train-frequency 10 Experiment results PR vwxyzjn/cleanrl#157 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/dqn . Below are the average episodic returns for dqn.py . Environment dqn.py CartPole-v1 471.21 \u00b1 43.45 Acrobot-v1 -93.37 \u00b1 8.46 MountainCar-v0 -170.51 \u00b1 26.22 Note that the DQN has no official benchmark on classic control environments, so we did not include a comparison. That said, our dqn.py was able to achieve near perfect scores in CartPole-v1 and Acrobot-v1 ; further, it can obtain successful runs in the sparse environment MountainCar-v0 . Learning curves: Tracked experiments and game play videos: Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through deep reinforcement learning. Nature 518, 529\u2013533 (2015). https://doi.org/10.1038/nature14236 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 [Proposal] Formal API handling of truncation vs termination. https://github.com/openai/gym/issues/2510 \u21a9 Hessel, M., Modayil, J., Hasselt, H.V., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M.G., & Silver, D. (2018). Rainbow: Combining Improvements in Deep Reinforcement Learning. AAAI. \u21a9","title":"Deep Q-Learning (DQN)"},{"location":"rl-algorithms/dqn/#deep-q-learning-dqn","text":"","title":"Deep Q-Learning (DQN)"},{"location":"rl-algorithms/dqn/#overview","text":"As an extension of the Q-learning, DQN's main technical contribution is the use of replay buffer and target network, both of which would help improve the stability of the algorithm. Original papers: Human-level control through deep reinforcement learning","title":"Overview"},{"location":"rl-algorithms/dqn/#implemented-variants","text":"Variants Implemented Description dqn_atari.py , docs For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. dqn.py , docs For classic control tasks like CartPole-v1 . Below are our single-file implementations of DQN:","title":"Implemented Variants"},{"location":"rl-algorithms/dqn/#dqn_ataripy","text":"The dqn_atari.py has the following features: For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space","title":"dqn_atari.py"},{"location":"rl-algorithms/dqn/#usage","text":"poetry install -E atari python cleanrl/dqn_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/dqn_atari.py --env-id PongNoFrameskip-v4","title":"Usage"},{"location":"rl-algorithms/dqn/#explanation-of-the-logged-metrics","text":"Running python cleanrl/dqn_atari.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/td_loss : the mean squared error (MSE) between the Q values at timestep \\(t\\) and the Bellman update target estimated using the reward \\(r_t\\) and the Q values at timestep \\(t+1\\) , thus minimizing the one-step temporal difference. Formally, it can be expressed by the equation below. $$ J(\\theta^{Q}) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\big[ (Q(s, a) - y)^2 \\big], $$ with the Bellman update target is \\(y = r + \\gamma \\, Q^{'}(s', a')\\) and the replay buffer is \\(\\mathcal{D}\\) . losses/q_values : implemented as qf1(data.observations, data.actions).view(-1) , it is the average Q values of the sampled data in the replay buffer; useful when gauging if under or over estimation happens.","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/dqn/#implementation-details","text":"dqn_atari.py is based on (Mnih et al., 2015) 1 but presents a few implementation differences: dqn_atari.py use slightly different hyperparameters. Specifically, dqn_atari.py uses the more popular Adam Optimizer with the --learning-rate=1e-4 as follows: optim . Adam ( q_network . parameters (), lr = 1e-4 ) whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses the RMSProp optimizer with --learning-rate=2.5e-4 , gradient momentum 0.95 , squared gradient momentum 0.95 , and min squared gradient 0.01 as follows: optim . RMSprop ( q_network . parameters (), lr = 2.5e-4 , momentum = 0.95 , # ... PyTorch's RMSprop does not directly support # squared gradient momentum and min squared gradient # so we are not sure what to put here. ) dqn_atari.py uses --learning-starts=80000 whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --learning-starts=50000 . dqn_atari.py uses --target-network-frequency=1000 whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --learning-starts=10000 . dqn_atari.py uses --total-timesteps=10000000 (i.e., 10M timesteps = 40M frames because of frame-skipping) whereas (Mnih et al., 2015) 1 uses --total-timesteps=50000000 (i.e., 50M timesteps = 200M frames) (See \"Training details\" under \"METHODS\" on page 6 and the related source code run_gpu#L32 , dqn/train_agent.lua#L81-L82 , and dqn/train_agent.lua#L165-L169 ). dqn_atari.py uses --end-e=0.01 (the final exploration epsilon) whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --end-e=0.1 . dqn_atari.py uses --exploration-fraction=0.1 whereas (Mnih et al., 2015) 1 (Exntended Data Table 1) uses --exploration-fraction=0.02 (all corresponds to 250000 steps or 1M frames being the frame that epsilon is annealed to --end-e=0.1 ). dqn_atari.py handles truncation and termination properly like (Mnih et al., 2015) 1 by using SB3's replay buffer's handle_timeout_termination=True . dqn_atari.py use a self-contained evaluation scheme: dqn_atari.py reports the episodic returns obtained throughout training, whereas (Mnih et al., 2015) 1 is trained with --end-e=0.1 but reported episodic returns using a separate evaluation process with --end-e=0.01 (See \"Evaluation procedure\" under \"METHODS\" on page 6). dqn_atari.py rescales the gradient so that the norm of the parameters does not exceed 0.5 like done in PPO ( ppo2/model.py#L102-L108 ).","title":"Implementation details"},{"location":"rl-algorithms/dqn/#experiment-results","text":"PR vwxyzjn/cleanrl#124 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/dqn . Below are the average episodic returns for dqn_atari.py . Environment dqn_atari.py 10M steps (Mnih et al., 2015) 1 50M steps (Hessel et al., 2017, Figure 5) 3 BreakoutNoFrameskip-v4 337.64 \u00b1 69.47 401.2 \u00b1 26.9 ~230 at 10M steps, ~300 at 50M steps PongNoFrameskip-v4 20.293 \u00b1 0.37 18.9 \u00b1 1.3 ~20 10M steps, ~20 at 50M steps BeamRiderNoFrameskip-v4 6207.41 \u00b1 1019.96 6846 \u00b1 1619 ~6000 10M steps, ~7000 at 50M steps Note that we save computational time by reducing timesteps from 50M to 10M, but our dqn_atari.py scores the same or higher than (Mnih et al., 2015) 1 in 10M steps. Learning curves: Tracked experiments and game play videos:","title":"Experiment results"},{"location":"rl-algorithms/dqn/#dqnpy","text":"The dqn.py has the following features: Works with the Box observation space of low-level features Works with the Discrete action space Works with envs like CartPole-v1","title":"dqn.py"},{"location":"rl-algorithms/dqn/#usage_1","text":"python cleanrl/dqn.py --env-id CartPole-v1","title":"Usage"},{"location":"rl-algorithms/dqn/#explanation-of-the-logged-metrics_1","text":"See related docs for dqn_atari.py .","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/dqn/#implementation-details_1","text":"The dqn.py shares the same implementation details as dqn_atari.py except the dqn.py runs with different hyperparameters and neural network architecture. Specifically, dqn.py uses a simpler neural network as follows: self . network = nn . Sequential ( nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 120 ), nn . ReLU (), nn . Linear ( 120 , 84 ), nn . ReLU (), nn . Linear ( 84 , env . single_action_space . n ), ) dqn.py runs with different hyperparameters: python dqn.py --total-timesteps 500000 \\ --learning-rate 2 .5e-4 \\ --buffer-size 10000 \\ --gamma 0 .99 \\ --target-network-frequency 500 \\ --max-grad-norm 0 .5 \\ --batch-size 128 \\ --start-e 1 \\ --end-e 0 .05 \\ --exploration-fraction 0 .5 \\ --learning-starts 10000 \\ --train-frequency 10","title":"Implementation details"},{"location":"rl-algorithms/dqn/#experiment-results_1","text":"PR vwxyzjn/cleanrl#157 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/dqn . Below are the average episodic returns for dqn.py . Environment dqn.py CartPole-v1 471.21 \u00b1 43.45 Acrobot-v1 -93.37 \u00b1 8.46 MountainCar-v0 -170.51 \u00b1 26.22 Note that the DQN has no official benchmark on classic control environments, so we did not include a comparison. That said, our dqn.py was able to achieve near perfect scores in CartPole-v1 and Acrobot-v1 ; further, it can obtain successful runs in the sparse environment MountainCar-v0 . Learning curves: Tracked experiments and game play videos: Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through deep reinforcement learning. Nature 518, 529\u2013533 (2015). https://doi.org/10.1038/nature14236 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 [Proposal] Formal API handling of truncation vs termination. https://github.com/openai/gym/issues/2510 \u21a9 Hessel, M., Modayil, J., Hasselt, H.V., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M.G., & Silver, D. (2018). Rainbow: Combining Improvements in Deep Reinforcement Learning. AAAI. \u21a9","title":"Experiment results"},{"location":"rl-algorithms/overview/","text":"Overview Algorithm Variants Implemented \u2705 Proximal Policy Gradient (PPO) ppo.py , docs ppo_atari.py , docs ppo_continuous_action.py , docs ppo_atari_lstm.py ppo_procgen.py \u2705 Deep Q-Learning (DQN) dqn.py dqn_atari.py \u2705 Categorical DQN (C51) c51.py c51_atari.py \u2705 Soft Actor-Critic (SAC) sac_continuous_action.py \u2705 Deep Deterministic Policy Gradient (DDPG) ddpg_continuous_action.py \u2705 Twin Delayed Deep Deterministic Policy Gradient (TD3) td3_continuous_action.py","title":"Overview"},{"location":"rl-algorithms/overview/#overview","text":"Algorithm Variants Implemented \u2705 Proximal Policy Gradient (PPO) ppo.py , docs ppo_atari.py , docs ppo_continuous_action.py , docs ppo_atari_lstm.py ppo_procgen.py \u2705 Deep Q-Learning (DQN) dqn.py dqn_atari.py \u2705 Categorical DQN (C51) c51.py c51_atari.py \u2705 Soft Actor-Critic (SAC) sac_continuous_action.py \u2705 Deep Deterministic Policy Gradient (DDPG) ddpg_continuous_action.py \u2705 Twin Delayed Deep Deterministic Policy Gradient (TD3) td3_continuous_action.py","title":"Overview"},{"location":"rl-algorithms/ppo/","text":"Proximal Policy Gradient (PPO) Overview PPO is one of the most popular DRL algorithms. It runs reasonably fast by leveraging vector (parallel) environments and naturally works well with different action spaces, therefore supporting a variety of games. It also has good sample efficiency compared to algorithms such as DQN. Original paper: Proximal Policy Optimization Algorithms Reference resources: Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study All our PPO implementations below are augmented with the same code-level optimizations presented in openai/baselines 's PPO . See The 32 Implementation Details of Proximal Policy Optimization (PPO) Algorithm for more details. Implemented Variants Variants Implemented Description ppo.py , docs For classic control tasks like CartPole-v1 . ppo_atari.py , docs For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. ppo_continuous_action.py , docs For continuous action space. Also implemented Mujoco-specific code-level optimizations Below are our single-file implementations of PPO: ppo.py The ppo.py has the following features: Works with the Box observation space of low-level features Works with the Discrete action space Works with envs like CartPole-v1 Usage poetry install python cleanrl/ppo.py --help python cleanrl/ppo.py --env-id CartPole-v1 Implementation details ppo.py includes the 11 core implementation details: Vectorized architecture ( common/cmd_util.py#L22 ) Orthogonal Initialization of Weights and Constant Initialization of biases ( a2c/utils.py#L58) ) The Adam Optimizer's Epsilon Parameter ( ppo2/model.py#L100 ) Adam Learning Rate Annealing ( ppo2/ppo2.py#L133-L135 ) Generalized Advantage Estimation ( ppo2/runner.py#L56-L65 ) Mini-batch Updates ( ppo2/ppo2.py#L157-L166 ) Normalization of Advantages ( ppo2/model.py#L139 ) Clipped surrogate objective ( ppo2/model.py#L81-L86 ) Value Function Loss Clipping ( ppo2/model.py#L68-L75 ) Overall Loss and Entropy Bonus ( ppo2/model.py#L91 ) Global Gradient Clipping ( ppo2/model.py#L102-L108 ) Experiment results PR vwxyzjn/cleanrl#120 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/ppo . Below are the average episodic returns for ppo.py . To ensure the quality of the implementation, we compared the results against openai/baselies ' PPO. Environment ppo.py openai/baselies ' PPO CartPole-v1 488.75 \u00b1 18.40 497.54 \u00b1 4.02 Acrobot-v1 -82.48 \u00b1 5.93 -81.82 \u00b1 5.58 MountainCar-v0 -200.00 \u00b1 0.00 -200.00 \u00b1 0.00 Learning curves: Tracked experiments and game play videos: Video tutorial If you'd like to learn ppo.py in-depth, consider checking out the following video tutorial: ppo_atari.py The ppo_atari.py has the following features: For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space Includes the 9 Atari-specific implementation details as shown in the following video tutorial ppo_continuous_action.py The ppo_continuous_action.py has the following features: For continuous action space. Also implemented Mujoco-specific code-level optimizations Works with the Box observation space of low-level features Works with the Box (continuous) action space Includes the 8 implementation details for as shown in the following video tutorial (need fixing)","title":"Proximal Policy Gradient (PPO)"},{"location":"rl-algorithms/ppo/#proximal-policy-gradient-ppo","text":"","title":"Proximal Policy Gradient (PPO)"},{"location":"rl-algorithms/ppo/#overview","text":"PPO is one of the most popular DRL algorithms. It runs reasonably fast by leveraging vector (parallel) environments and naturally works well with different action spaces, therefore supporting a variety of games. It also has good sample efficiency compared to algorithms such as DQN. Original paper: Proximal Policy Optimization Algorithms Reference resources: Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study All our PPO implementations below are augmented with the same code-level optimizations presented in openai/baselines 's PPO . See The 32 Implementation Details of Proximal Policy Optimization (PPO) Algorithm for more details.","title":"Overview"},{"location":"rl-algorithms/ppo/#implemented-variants","text":"Variants Implemented Description ppo.py , docs For classic control tasks like CartPole-v1 . ppo_atari.py , docs For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. ppo_continuous_action.py , docs For continuous action space. Also implemented Mujoco-specific code-level optimizations Below are our single-file implementations of PPO:","title":"Implemented Variants"},{"location":"rl-algorithms/ppo/#ppopy","text":"The ppo.py has the following features: Works with the Box observation space of low-level features Works with the Discrete action space Works with envs like CartPole-v1","title":"ppo.py"},{"location":"rl-algorithms/ppo/#usage","text":"poetry install python cleanrl/ppo.py --help python cleanrl/ppo.py --env-id CartPole-v1","title":"Usage"},{"location":"rl-algorithms/ppo/#implementation-details","text":"ppo.py includes the 11 core implementation details: Vectorized architecture ( common/cmd_util.py#L22 ) Orthogonal Initialization of Weights and Constant Initialization of biases ( a2c/utils.py#L58) ) The Adam Optimizer's Epsilon Parameter ( ppo2/model.py#L100 ) Adam Learning Rate Annealing ( ppo2/ppo2.py#L133-L135 ) Generalized Advantage Estimation ( ppo2/runner.py#L56-L65 ) Mini-batch Updates ( ppo2/ppo2.py#L157-L166 ) Normalization of Advantages ( ppo2/model.py#L139 ) Clipped surrogate objective ( ppo2/model.py#L81-L86 ) Value Function Loss Clipping ( ppo2/model.py#L68-L75 ) Overall Loss and Entropy Bonus ( ppo2/model.py#L91 ) Global Gradient Clipping ( ppo2/model.py#L102-L108 )","title":"Implementation details"},{"location":"rl-algorithms/ppo/#experiment-results","text":"PR vwxyzjn/cleanrl#120 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/ppo . Below are the average episodic returns for ppo.py . To ensure the quality of the implementation, we compared the results against openai/baselies ' PPO. Environment ppo.py openai/baselies ' PPO CartPole-v1 488.75 \u00b1 18.40 497.54 \u00b1 4.02 Acrobot-v1 -82.48 \u00b1 5.93 -81.82 \u00b1 5.58 MountainCar-v0 -200.00 \u00b1 0.00 -200.00 \u00b1 0.00 Learning curves: Tracked experiments and game play videos:","title":"Experiment results"},{"location":"rl-algorithms/ppo/#video-tutorial","text":"If you'd like to learn ppo.py in-depth, consider checking out the following video tutorial:","title":"Video tutorial"},{"location":"rl-algorithms/ppo/#ppo_ataripy","text":"The ppo_atari.py has the following features: For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discrete action space Includes the 9 Atari-specific implementation details as shown in the following video tutorial","title":"ppo_atari.py"},{"location":"rl-algorithms/ppo/#ppo_continuous_actionpy","text":"The ppo_continuous_action.py has the following features: For continuous action space. Also implemented Mujoco-specific code-level optimizations Works with the Box observation space of low-level features Works with the Box (continuous) action space Includes the 8 implementation details for as shown in the following video tutorial (need fixing)","title":"ppo_continuous_action.py"},{"location":"rl-algorithms/sac/","text":"Soft Actor-Critic (SAC) Overview The Soft Actor-Critic (SAC) algorithm extends the DDPG algorithm by 1) using a stochastic policy, which in theory can express multi-modal optimal policies. This also enables the use of 2) entropy regularization based on the stochastic policy's entropy. It serves as a built-in, state-dependent exploration heuristic for the agent, instead of relying on non-correlated noise processes as in DDPG , or TD3 Additionally, it incorporates the 3) usage of two Soft Q-network to reduce the overestimation bias issue in Q-network-based methods. Original papers: The SAC algorithm's initial proposal, and later updates and improvements can be chronologically traced through the following publications: Reinforcement Learning with Deep Energy-Based Policies Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor Composable Deep Reinforcement Learning for Robotic Manipulation Soft Actor-Critic Algorithms and Applications Reference resources: haarnoja/sac openai/spinningup pranz24/pytorch-soft-actor-critic DLR-RM/stable-baselines3 denisyarats/pytorch_sac haarnoja/softqlearning rail-berkeley/softlearning Variants Implemented Description sac_continuous_actions.py , docs For continuous action space Below is our single-file implementations of SAC: sac_continuous_action.py The sac_continuous_action.py has the following features: For continuous action space. Works with the Box observation space of low-level features. Works with the Box (continuous) action space. Numerically stable stochastic policy based on openai/spinningup and pranz24/pytorch-soft-actor-critic implementations. Supports automatic entropy coefficient \\(\\alpha\\) tuning, enabled by default. Usage poetry install # Pybullet poetry install -E pybullet ## Default python cleanrl/sac_continuous_action.py --env-id HopperBulletEnv-v0 ## Without Automatic entropy coef. tuning python cleanrl/sac_continuous_action.py --env-id HopperBulletEnv-v0 --autotune False --alpha 0 .2 Explanation of the logged metrics Running python cleanrl/ddpg_continuous_action.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : the episodic return of the game during training charts/SPS : number of steps per second losses/qf1_loss , losses/qf2_loss : for each Soft Q-value network \\(Q_{\\theta_i}\\) , \\(i \\in \\{1,2\\}\\) , this metric holds the mean squared error (MSE) between the soft Q-value estimate \\(Q_{\\theta_i}(s, a)\\) and the entropy regularized Bellman update target estimated as \\(r_t + \\gamma \\, Q_{\\theta_{i}^{'}}(s', a') + \\alpha \\, \\mathcal{H} \\big[ \\pi(a' \\vert s') \\big]\\) . More formally, the Soft Q-value loss for the \\(i\\) -th network is obtained by: \\[ J(\\theta^{Q}_{i}) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\big[ (Q_{\\theta_i}(s, a) - y)^2 \\big] \\] with the entropy regularized , Soft Bellman update target : $$ y = r(s, a) + \\gamma ({\\color{orange} \\min_{\\theta_{1,2}}Q_{\\theta_i^{'}}(s',a')} - \\alpha \\, \\text{log} \\pi( \\cdot \\vert s')) $$ where \\(a' \\sim \\pi( \\cdot \\vert s')\\) , \\(\\text{log} \\pi( \\cdot \\vert s')\\) approximates the entropy of the policy, and \\(\\mathcal{D}\\) is the replay buffer storing samples of the agent during training. Here, \\(\\min_{\\theta_{1,2}}Q_{\\theta_i^{'}}(s',a')\\) takes the minimum Soft Q-value network estimate between the two target Q-value networks \\(Q_{\\theta_1^{'}}\\) and \\(Q_{\\theta_2^{'}}\\) for the next state and action pair, so as to reduce over-estimation bias. losses/qf_loss : averages losses/qf1_loss and losses/qf2_loss for comparison with algorithms using a single Q-value network. losses/actor_loss : Given the stochastic nature of the policy in SAC, the actor (or policy) objective is formulated so as to maximize the likelihood of actions \\(a \\sim \\pi( \\cdot \\vert s)\\) that would result in high Q-value estimate \\(Q(s, a)\\) . Additionally, the policy objective encourages the policy to maintain its entropy high enough to help explore, discover, and capture multi-modal optimal policies. The policy's objective function can thus be defined as: \\[ \\text{max}_{\\phi} \\, J_{\\pi}(\\phi) = \\mathbb{E}_{s \\sim \\mathcal{D}} \\Big[ \\text{min}_{i=1,2} Q_{\\theta_i}(s, a) - \\alpha \\, \\text{log}\\pi_{\\phi}(a \\vert s) \\Big] \\] where the action is sampled using the reparameterization trick 1 : \\(a = \\mu_{\\phi}(s) + \\epsilon \\, \\sigma_{\\phi}(s)\\) with \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\) , \\(\\text{log} \\pi_{\\phi}( \\cdot \\vert s')\\) approximates the entropy of the policy, and \\(\\mathcal{D}\\) is the replay buffer storing samples of the agent during training. losses/alpha : \\(\\alpha\\) coefficient for entropy regularization of the policy. losses/alpha_loss : In the policy's objective defined above, the coefficient of the entropy bonus \\(\\alpha\\) is kept fixed all across the training. As suggested by the authors in Section 5 of the Soft Actor-Critic And Applications paper, the original purpose of augmenting the standard reward with the entropy of the policy is to encourage exploration of not well enough explored states (thus high entropy). Conversely, for states where the policy has already learned a near-optimal policy, it would be preferable to reduce the entropy bonus of the policy, so that it does not become sub-optimal due to the entropy maximization incentive . Therefore, having a fixed value for \\(\\alpha\\) does not fit this desideratum of matching the entropy bonus with the knowledge of the policy at an arbitrary state during its training. To mitigate this, the authors proposed a method to dynamically adjust \\(\\alpha\\) as the policy is trained, which is as follows: \\[ \\alpha^{*}_t = \\text{argmin}_{\\alpha_t} \\mathbb{E}_{a_t \\sim \\pi^{*}_t} \\big[ -\\alpha_t \\, \\text{log}\\pi^{*}_t(a_t \\vert s_t; \\alpha_t) - \\alpha_t \\mathcal{H} \\big], \\] where \\(\\mathcal{H}\\) represents the target entropy , the desired lower bound for the expected entropy of the policy over the trajectory distribution induced by the latter. As a heuristic for the target entropy , the authors use the dimension of the action space of the task. Implementation details CleanRL's sac_continuous_action.py implementation is based on openai/spinningup . sac_continuous_action.py uses a numerically stable estimation method for the standard deviation \\(\\sigma\\) of the policy, which squashes it into a range of reasonable values for a standard deviation: LOG_STD_MAX = 2 LOG_STD_MIN = - 5 class Actor ( nn . Module ): def __init__ ( self , env ): super ( Actor , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 256 ) self . fc2 = nn . Linear ( 256 , 256 ) self . fc_mean = nn . Linear ( 256 , np . prod ( env . single_action_space . shape )) self . fc_logstd = nn . Linear ( 256 , np . prod ( env . single_action_space . shape )) # action rescaling self . action_scale = torch . FloatTensor (( env . action_space . high - env . action_space . low ) / 2.0 ) self . action_bias = torch . FloatTensor (( env . action_space . high + env . action_space . low ) / 2.0 ) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) mean = self . fc_mean ( x ) log_std = self . fc_logstd ( x ) log_std = torch . tanh ( log_std ) log_std = LOG_STD_MIN + 0.5 * ( LOG_STD_MAX - LOG_STD_MIN ) * ( log_std + 1 ) # From SpinUp / Denis Yarats return mean , log_std def get_action ( self , x ): mean , log_std = self ( x ) std = log_std . exp () normal = torch . distributions . Normal ( mean , std ) x_t = normal . rsample () # for reparameterization trick (mean + std * N(0,1)) y_t = torch . tanh ( x_t ) action = y_t * self . action_scale + self . action_bias log_prob = normal . log_prob ( x_t ) # Enforcing Action Bound log_prob -= torch . log ( self . action_scale * ( 1 - y_t . pow ( 2 )) + 1e-6 ) log_prob = log_prob . sum ( 1 , keepdim = True ) mean = torch . tanh ( mean ) * self . action_scale + self . action_bias return action , log_prob , mean def to ( self , device ): self . action_scale = self . action_scale . to ( device ) self . action_bias = self . action_bias . to ( device ) return super ( Actor , self ) . to ( device ) Note that unlike openai/spinningup 's implementation which uses LOG_STD_MIN = -20 , CleanRL's uses LOG_STD_MIN = -5 instead. sac_continuous_action.py uses different learning rates for the policy and the Soft Q-value networks optimization. parser . add_argument ( \"--policy-lr\" , type = float , default = 3e-4 , help = \"the learning rate of the policy network optimizer\" ) parser . add_argument ( \"--q-lr\" , type = float , default = 1e-3 , help = \"the learning rate of the Q network network optimizer\" ) while openai/spinningup 's uses a single learning rate of lr=1e-3 for both components. Note that in case it is used, the automatic entropy coefficient \\(\\alpha\\) 's tuning shares the q-lr learning rate: # Automatic entropy tuning if args . autotune : target_entropy = - torch . prod ( torch . Tensor ( envs . single_action_space . shape ) . to ( device )) . item () log_alpha = torch . zeros ( 1 , requires_grad = True , device = device ) alpha = log_alpha . exp () . item () a_optimizer = optim . Adam ([ log_alpha ], lr = args . q_lr ) else : alpha = args . alpha sac_continuous_action.py uses --batch-size=256 while openai/spinningup 's uses --batch-size=100 by default. sac_continuous_action.py also implementts global gradient norm clipping with --max-grad-norm set to 0.5 by default. parser . add_argument ( \"--max-grad-norm\" , type = float , default = 0.5 , help = \"the maximum norm for the gradient clipping\" ) The gradient norm clipping is applied during the Soft Q-value, and the policy networks optimization: qf_loss = qf1_loss + qf2_loss q_optimizer . zero_grad () qf_loss . backward () nn . utils . clip_grad_norm_ ( list ( qf1 . parameters ()) + list ( qf2 . parameters ()), args . max_grad_norm ) q_optimizer . step () actor_loss = (( alpha * log_pi ) - min_qf_pi ) . mean () actor_optimizer . zero_grad () actor_loss . backward () nn . utils . clip_grad_norm_ ( list ( actor . parameters ()), args . max_grad_norm ) actor_optimizer . step () Experiment results PR vwxyzjn/cleanrl#146 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/sac , after 1 million training steps. The table below compares the results of CleanRL's sac_continuous_action.py with the latest published results by the original authors of the SAC algorithm. Info Note that the results table above references the training episodic return for sac_continuous_action.py , the results of Soft Actor-Critic Algorithms and Applications reference evaluation episodic return obtained by running the policy in the deterministic mode. Environment sac_continuous_action.py SAC: Algorithms and Applications @ 1M steps HalfCheetah-v2 9,063 \u00b1 1381 ~11,250 Walker2d-v2 4554 \u00b1 296 ~4,800 Hopper-v2 2347 \u00b1 538 ~3,250 Learning curves Tracked experiments and gameplay videos Diederik P Kingma, Max Welling (2016). Auto-Encoding Variational Bayes. ArXiv, abs/1312.6114. https://arxiv.org/abs/1312.6114 \u21a9","title":"Soft Actor-Critic (SAC)"},{"location":"rl-algorithms/sac/#soft-actor-critic-sac","text":"","title":"Soft Actor-Critic (SAC)"},{"location":"rl-algorithms/sac/#overview","text":"The Soft Actor-Critic (SAC) algorithm extends the DDPG algorithm by 1) using a stochastic policy, which in theory can express multi-modal optimal policies. This also enables the use of 2) entropy regularization based on the stochastic policy's entropy. It serves as a built-in, state-dependent exploration heuristic for the agent, instead of relying on non-correlated noise processes as in DDPG , or TD3 Additionally, it incorporates the 3) usage of two Soft Q-network to reduce the overestimation bias issue in Q-network-based methods. Original papers: The SAC algorithm's initial proposal, and later updates and improvements can be chronologically traced through the following publications: Reinforcement Learning with Deep Energy-Based Policies Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor Composable Deep Reinforcement Learning for Robotic Manipulation Soft Actor-Critic Algorithms and Applications Reference resources: haarnoja/sac openai/spinningup pranz24/pytorch-soft-actor-critic DLR-RM/stable-baselines3 denisyarats/pytorch_sac haarnoja/softqlearning rail-berkeley/softlearning Variants Implemented Description sac_continuous_actions.py , docs For continuous action space Below is our single-file implementations of SAC:","title":"Overview"},{"location":"rl-algorithms/sac/#sac_continuous_actionpy","text":"The sac_continuous_action.py has the following features: For continuous action space. Works with the Box observation space of low-level features. Works with the Box (continuous) action space. Numerically stable stochastic policy based on openai/spinningup and pranz24/pytorch-soft-actor-critic implementations. Supports automatic entropy coefficient \\(\\alpha\\) tuning, enabled by default.","title":"sac_continuous_action.py"},{"location":"rl-algorithms/sac/#usage","text":"poetry install # Pybullet poetry install -E pybullet ## Default python cleanrl/sac_continuous_action.py --env-id HopperBulletEnv-v0 ## Without Automatic entropy coef. tuning python cleanrl/sac_continuous_action.py --env-id HopperBulletEnv-v0 --autotune False --alpha 0 .2","title":"Usage"},{"location":"rl-algorithms/sac/#explanation-of-the-logged-metrics","text":"Running python cleanrl/ddpg_continuous_action.py will automatically record various metrics such as actor or value losses in Tensorboard. Below is the documentation for these metrics: charts/episodic_return : the episodic return of the game during training charts/SPS : number of steps per second losses/qf1_loss , losses/qf2_loss : for each Soft Q-value network \\(Q_{\\theta_i}\\) , \\(i \\in \\{1,2\\}\\) , this metric holds the mean squared error (MSE) between the soft Q-value estimate \\(Q_{\\theta_i}(s, a)\\) and the entropy regularized Bellman update target estimated as \\(r_t + \\gamma \\, Q_{\\theta_{i}^{'}}(s', a') + \\alpha \\, \\mathcal{H} \\big[ \\pi(a' \\vert s') \\big]\\) . More formally, the Soft Q-value loss for the \\(i\\) -th network is obtained by: \\[ J(\\theta^{Q}_{i}) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\big[ (Q_{\\theta_i}(s, a) - y)^2 \\big] \\] with the entropy regularized , Soft Bellman update target : $$ y = r(s, a) + \\gamma ({\\color{orange} \\min_{\\theta_{1,2}}Q_{\\theta_i^{'}}(s',a')} - \\alpha \\, \\text{log} \\pi( \\cdot \\vert s')) $$ where \\(a' \\sim \\pi( \\cdot \\vert s')\\) , \\(\\text{log} \\pi( \\cdot \\vert s')\\) approximates the entropy of the policy, and \\(\\mathcal{D}\\) is the replay buffer storing samples of the agent during training. Here, \\(\\min_{\\theta_{1,2}}Q_{\\theta_i^{'}}(s',a')\\) takes the minimum Soft Q-value network estimate between the two target Q-value networks \\(Q_{\\theta_1^{'}}\\) and \\(Q_{\\theta_2^{'}}\\) for the next state and action pair, so as to reduce over-estimation bias. losses/qf_loss : averages losses/qf1_loss and losses/qf2_loss for comparison with algorithms using a single Q-value network. losses/actor_loss : Given the stochastic nature of the policy in SAC, the actor (or policy) objective is formulated so as to maximize the likelihood of actions \\(a \\sim \\pi( \\cdot \\vert s)\\) that would result in high Q-value estimate \\(Q(s, a)\\) . Additionally, the policy objective encourages the policy to maintain its entropy high enough to help explore, discover, and capture multi-modal optimal policies. The policy's objective function can thus be defined as: \\[ \\text{max}_{\\phi} \\, J_{\\pi}(\\phi) = \\mathbb{E}_{s \\sim \\mathcal{D}} \\Big[ \\text{min}_{i=1,2} Q_{\\theta_i}(s, a) - \\alpha \\, \\text{log}\\pi_{\\phi}(a \\vert s) \\Big] \\] where the action is sampled using the reparameterization trick 1 : \\(a = \\mu_{\\phi}(s) + \\epsilon \\, \\sigma_{\\phi}(s)\\) with \\(\\epsilon \\sim \\mathcal{N}(0, 1)\\) , \\(\\text{log} \\pi_{\\phi}( \\cdot \\vert s')\\) approximates the entropy of the policy, and \\(\\mathcal{D}\\) is the replay buffer storing samples of the agent during training. losses/alpha : \\(\\alpha\\) coefficient for entropy regularization of the policy. losses/alpha_loss : In the policy's objective defined above, the coefficient of the entropy bonus \\(\\alpha\\) is kept fixed all across the training. As suggested by the authors in Section 5 of the Soft Actor-Critic And Applications paper, the original purpose of augmenting the standard reward with the entropy of the policy is to encourage exploration of not well enough explored states (thus high entropy). Conversely, for states where the policy has already learned a near-optimal policy, it would be preferable to reduce the entropy bonus of the policy, so that it does not become sub-optimal due to the entropy maximization incentive . Therefore, having a fixed value for \\(\\alpha\\) does not fit this desideratum of matching the entropy bonus with the knowledge of the policy at an arbitrary state during its training. To mitigate this, the authors proposed a method to dynamically adjust \\(\\alpha\\) as the policy is trained, which is as follows: \\[ \\alpha^{*}_t = \\text{argmin}_{\\alpha_t} \\mathbb{E}_{a_t \\sim \\pi^{*}_t} \\big[ -\\alpha_t \\, \\text{log}\\pi^{*}_t(a_t \\vert s_t; \\alpha_t) - \\alpha_t \\mathcal{H} \\big], \\] where \\(\\mathcal{H}\\) represents the target entropy , the desired lower bound for the expected entropy of the policy over the trajectory distribution induced by the latter. As a heuristic for the target entropy , the authors use the dimension of the action space of the task.","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/sac/#implementation-details","text":"CleanRL's sac_continuous_action.py implementation is based on openai/spinningup . sac_continuous_action.py uses a numerically stable estimation method for the standard deviation \\(\\sigma\\) of the policy, which squashes it into a range of reasonable values for a standard deviation: LOG_STD_MAX = 2 LOG_STD_MIN = - 5 class Actor ( nn . Module ): def __init__ ( self , env ): super ( Actor , self ) . __init__ () self . fc1 = nn . Linear ( np . array ( env . single_observation_space . shape ) . prod (), 256 ) self . fc2 = nn . Linear ( 256 , 256 ) self . fc_mean = nn . Linear ( 256 , np . prod ( env . single_action_space . shape )) self . fc_logstd = nn . Linear ( 256 , np . prod ( env . single_action_space . shape )) # action rescaling self . action_scale = torch . FloatTensor (( env . action_space . high - env . action_space . low ) / 2.0 ) self . action_bias = torch . FloatTensor (( env . action_space . high + env . action_space . low ) / 2.0 ) def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) mean = self . fc_mean ( x ) log_std = self . fc_logstd ( x ) log_std = torch . tanh ( log_std ) log_std = LOG_STD_MIN + 0.5 * ( LOG_STD_MAX - LOG_STD_MIN ) * ( log_std + 1 ) # From SpinUp / Denis Yarats return mean , log_std def get_action ( self , x ): mean , log_std = self ( x ) std = log_std . exp () normal = torch . distributions . Normal ( mean , std ) x_t = normal . rsample () # for reparameterization trick (mean + std * N(0,1)) y_t = torch . tanh ( x_t ) action = y_t * self . action_scale + self . action_bias log_prob = normal . log_prob ( x_t ) # Enforcing Action Bound log_prob -= torch . log ( self . action_scale * ( 1 - y_t . pow ( 2 )) + 1e-6 ) log_prob = log_prob . sum ( 1 , keepdim = True ) mean = torch . tanh ( mean ) * self . action_scale + self . action_bias return action , log_prob , mean def to ( self , device ): self . action_scale = self . action_scale . to ( device ) self . action_bias = self . action_bias . to ( device ) return super ( Actor , self ) . to ( device ) Note that unlike openai/spinningup 's implementation which uses LOG_STD_MIN = -20 , CleanRL's uses LOG_STD_MIN = -5 instead. sac_continuous_action.py uses different learning rates for the policy and the Soft Q-value networks optimization. parser . add_argument ( \"--policy-lr\" , type = float , default = 3e-4 , help = \"the learning rate of the policy network optimizer\" ) parser . add_argument ( \"--q-lr\" , type = float , default = 1e-3 , help = \"the learning rate of the Q network network optimizer\" ) while openai/spinningup 's uses a single learning rate of lr=1e-3 for both components. Note that in case it is used, the automatic entropy coefficient \\(\\alpha\\) 's tuning shares the q-lr learning rate: # Automatic entropy tuning if args . autotune : target_entropy = - torch . prod ( torch . Tensor ( envs . single_action_space . shape ) . to ( device )) . item () log_alpha = torch . zeros ( 1 , requires_grad = True , device = device ) alpha = log_alpha . exp () . item () a_optimizer = optim . Adam ([ log_alpha ], lr = args . q_lr ) else : alpha = args . alpha sac_continuous_action.py uses --batch-size=256 while openai/spinningup 's uses --batch-size=100 by default. sac_continuous_action.py also implementts global gradient norm clipping with --max-grad-norm set to 0.5 by default. parser . add_argument ( \"--max-grad-norm\" , type = float , default = 0.5 , help = \"the maximum norm for the gradient clipping\" ) The gradient norm clipping is applied during the Soft Q-value, and the policy networks optimization: qf_loss = qf1_loss + qf2_loss q_optimizer . zero_grad () qf_loss . backward () nn . utils . clip_grad_norm_ ( list ( qf1 . parameters ()) + list ( qf2 . parameters ()), args . max_grad_norm ) q_optimizer . step () actor_loss = (( alpha * log_pi ) - min_qf_pi ) . mean () actor_optimizer . zero_grad () actor_loss . backward () nn . utils . clip_grad_norm_ ( list ( actor . parameters ()), args . max_grad_norm ) actor_optimizer . step ()","title":"Implementation details"},{"location":"rl-algorithms/sac/#experiment-results","text":"PR vwxyzjn/cleanrl#146 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/sac , after 1 million training steps. The table below compares the results of CleanRL's sac_continuous_action.py with the latest published results by the original authors of the SAC algorithm. Info Note that the results table above references the training episodic return for sac_continuous_action.py , the results of Soft Actor-Critic Algorithms and Applications reference evaluation episodic return obtained by running the policy in the deterministic mode. Environment sac_continuous_action.py SAC: Algorithms and Applications @ 1M steps HalfCheetah-v2 9,063 \u00b1 1381 ~11,250 Walker2d-v2 4554 \u00b1 296 ~4,800 Hopper-v2 2347 \u00b1 538 ~3,250","title":"Experiment results"},{"location":"rl-algorithms/sac/#learning-curves","text":"","title":"Learning curves"},{"location":"rl-algorithms/sac/#tracked-experiments-and-gameplay-videos","text":"Diederik P Kingma, Max Welling (2016). Auto-Encoding Variational Bayes. ArXiv, abs/1312.6114. https://arxiv.org/abs/1312.6114 \u21a9","title":"Tracked experiments and gameplay videos"},{"location":"rl-algorithms/td3/","text":"Twin Delayed Deep Deterministic Policy Gradient (TD3) Overview TD3 is a popular DRL algorithm for continuous control. It extends DDPG with three techniques: 1) Clipped Double Q-Learning, 2) Delayed Policy Updates, and 3) Target Policy Smoothing Regularization. With these three techniques TD3 shows significantly better performance compared to DDPG. Original paper: Addressing Function Approximation Error in Actor-Critic Methods Reference resources: sfujim/TD3 Twin Delayed DDPG | Spinning Up in Deep RL Implemented Variants Variants Implemented Description td3_continuous_action.py , docs For continuous action space Below are our single-file implementations of TD3: td3_continuous_action.py The td3_continuous_action.py has the following features: For continuous action space Works with the Box observation space of low-level features Works with the Box (continuous) action space Usage poetry install poetry install -E pybullet python cleanrl/td3_continuous_action.py --help python cleanrl/td3_continuous_action.py --env-id HopperBulletEnv-v0 poetry install -E mujoco # only works in Linux python cleanrl/td3_continuous_action.py --env-id Hopper-v3 Explanation of the logged metrics Running python cleanrl/td3_continuous_action.py will automatically record various metrics such as various losses in Tensorboard. Below are the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/qf1_loss : the MSE between the Q values at timestep \\(t\\) and the target Q values at timestep \\(t+1\\) , which minimizes temporal difference. losses/actor_loss : implemented as -qf1(data.observations, actor(data.observations)).mean() ; it is the negative average Q values calculated based on the 1) observations and the 2) actions computed by the actor based on these observations. By minimizing actor_loss , the optimizer updates the actors parameter using the following gradient (Fujimoto et al., 2018, Algorithm 1) 2 : \\[ \\nabla_{\\phi} J(\\phi)=\\left.N^{-1} \\sum \\nabla_{a} Q_{\\theta_{1}}(s, a)\\right|_{a=\\pi_{\\phi}(s)} \\nabla_{\\phi} \\pi_{\\phi}(s) \\] losses/qf1_values : implemented as `qf1(data.observations, data.actions).view(-1); it is the average Q values of the sampled data in the replay buffer; useful when gauging if under or over esitmations happen Implementation details Our td3_continuous_action.py is based on the TD3.py from sfujim/TD3 . Our td3_continuous_action.py presents the following implementation differences. td3_continuous_action.py uses a two separate objects qf1 and qf2 to represents the two Q functions in the Clipped Double Q-learning architecture, whereas TD3.py (Fujimoto et al., 2018) 2 uses a single Critic class that contains both Q networks. That said, these two implementations are virtually the same. td3_continuous_action.py rescales the gradient so that the norm of the parameters does not exceed 0.5 like done in PPO ( ppo2/model.py#L102-L108 ). Experiment results PR vwxyzjn/cleanrl#141 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/td3 . Below are the average episodic returns for td3_continuous_action.py (3 random seeds). To ensure the quality of the implementation, we compared the results against (Fujimoto et al., 2018) 2 . Environment td3_continuous_action.py TD3.py (Fujimoto et al., 2018, Table 1) 2 HalfCheetah 9391.52 \u00b1 448.54 9636.95 \u00b1 859.065 Walker2d 3895.80 \u00b1 333.89 4682.82 \u00b1 539.64 Hopper 3379.25 \u00b1 200.22 3564.07 \u00b1 114.74 Info Note that td3_continuous_action.py uses gym MuJoCo v2 environments while TD3.py (Fujimoto et al., 2018) 2 uses the gym MuJoCo v1 environments. According to the openai/gym#834 , gym MuJoCo v2 environments should be equivalent to the gym MuJoCo v1 environments. Also note the performance of our td3_continuous_action.py seems to be worse than the reference implementation on Walker2d. This is likely due to openai/gym#938 . We would have a hard time reproducing gym MuJoCo v1 environments because they have been long deprecated. One other thing could cause the performance difference: the original code reported the average episodic return using determinisitc evaluation (i.e., without exploration noise), see sfujim/TD3/main.py#L15-L32 , whereas we reported the episodic return during training and the policy gets updated between environments steps. Learning curves: Tracked experiments and game play videos: Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N.M., Erez, T., Tassa, Y., Silver, D., & Wierstra, D. (2016). Continuous control with deep reinforcement learning. CoRR, abs/1509.02971. https://arxiv.org/abs/1509.02971 \u21a9 Fujimoto, S., Hoof, H.V., & Meger, D. (2018). Addressing Function Approximation Error in Actor-Critic Methods. ArXiv, abs/1802.09477. https://arxiv.org/abs/1802.09477 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9","title":"Twin Delayed Deep Deterministic Policy Gradient (TD3)"},{"location":"rl-algorithms/td3/#twin-delayed-deep-deterministic-policy-gradient-td3","text":"","title":"Twin Delayed Deep Deterministic Policy Gradient (TD3)"},{"location":"rl-algorithms/td3/#overview","text":"TD3 is a popular DRL algorithm for continuous control. It extends DDPG with three techniques: 1) Clipped Double Q-Learning, 2) Delayed Policy Updates, and 3) Target Policy Smoothing Regularization. With these three techniques TD3 shows significantly better performance compared to DDPG. Original paper: Addressing Function Approximation Error in Actor-Critic Methods Reference resources: sfujim/TD3 Twin Delayed DDPG | Spinning Up in Deep RL","title":"Overview"},{"location":"rl-algorithms/td3/#implemented-variants","text":"Variants Implemented Description td3_continuous_action.py , docs For continuous action space Below are our single-file implementations of TD3:","title":"Implemented Variants"},{"location":"rl-algorithms/td3/#td3_continuous_actionpy","text":"The td3_continuous_action.py has the following features: For continuous action space Works with the Box observation space of low-level features Works with the Box (continuous) action space","title":"td3_continuous_action.py"},{"location":"rl-algorithms/td3/#usage","text":"poetry install poetry install -E pybullet python cleanrl/td3_continuous_action.py --help python cleanrl/td3_continuous_action.py --env-id HopperBulletEnv-v0 poetry install -E mujoco # only works in Linux python cleanrl/td3_continuous_action.py --env-id Hopper-v3","title":"Usage"},{"location":"rl-algorithms/td3/#explanation-of-the-logged-metrics","text":"Running python cleanrl/td3_continuous_action.py will automatically record various metrics such as various losses in Tensorboard. Below are the documentation for these metrics: charts/episodic_return : episodic return of the game charts/SPS : number of steps per second losses/qf1_loss : the MSE between the Q values at timestep \\(t\\) and the target Q values at timestep \\(t+1\\) , which minimizes temporal difference. losses/actor_loss : implemented as -qf1(data.observations, actor(data.observations)).mean() ; it is the negative average Q values calculated based on the 1) observations and the 2) actions computed by the actor based on these observations. By minimizing actor_loss , the optimizer updates the actors parameter using the following gradient (Fujimoto et al., 2018, Algorithm 1) 2 : \\[ \\nabla_{\\phi} J(\\phi)=\\left.N^{-1} \\sum \\nabla_{a} Q_{\\theta_{1}}(s, a)\\right|_{a=\\pi_{\\phi}(s)} \\nabla_{\\phi} \\pi_{\\phi}(s) \\] losses/qf1_values : implemented as `qf1(data.observations, data.actions).view(-1); it is the average Q values of the sampled data in the replay buffer; useful when gauging if under or over esitmations happen","title":"Explanation of the logged metrics"},{"location":"rl-algorithms/td3/#implementation-details","text":"Our td3_continuous_action.py is based on the TD3.py from sfujim/TD3 . Our td3_continuous_action.py presents the following implementation differences. td3_continuous_action.py uses a two separate objects qf1 and qf2 to represents the two Q functions in the Clipped Double Q-learning architecture, whereas TD3.py (Fujimoto et al., 2018) 2 uses a single Critic class that contains both Q networks. That said, these two implementations are virtually the same. td3_continuous_action.py rescales the gradient so that the norm of the parameters does not exceed 0.5 like done in PPO ( ppo2/model.py#L102-L108 ).","title":"Implementation details"},{"location":"rl-algorithms/td3/#experiment-results","text":"PR vwxyzjn/cleanrl#141 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/td3 . Below are the average episodic returns for td3_continuous_action.py (3 random seeds). To ensure the quality of the implementation, we compared the results against (Fujimoto et al., 2018) 2 . Environment td3_continuous_action.py TD3.py (Fujimoto et al., 2018, Table 1) 2 HalfCheetah 9391.52 \u00b1 448.54 9636.95 \u00b1 859.065 Walker2d 3895.80 \u00b1 333.89 4682.82 \u00b1 539.64 Hopper 3379.25 \u00b1 200.22 3564.07 \u00b1 114.74 Info Note that td3_continuous_action.py uses gym MuJoCo v2 environments while TD3.py (Fujimoto et al., 2018) 2 uses the gym MuJoCo v1 environments. According to the openai/gym#834 , gym MuJoCo v2 environments should be equivalent to the gym MuJoCo v1 environments. Also note the performance of our td3_continuous_action.py seems to be worse than the reference implementation on Walker2d. This is likely due to openai/gym#938 . We would have a hard time reproducing gym MuJoCo v1 environments because they have been long deprecated. One other thing could cause the performance difference: the original code reported the average episodic return using determinisitc evaluation (i.e., without exploration noise), see sfujim/TD3/main.py#L15-L32 , whereas we reported the episodic return during training and the policy gets updated between environments steps. Learning curves: Tracked experiments and game play videos: Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N.M., Erez, T., Tassa, Y., Silver, D., & Wierstra, D. (2016). Continuous control with deep reinforcement learning. CoRR, abs/1509.02971. https://arxiv.org/abs/1509.02971 \u21a9 Fujimoto, S., Hoof, H.V., & Meger, D. (2018). Addressing Function Approximation Error in Actor-Critic Methods. ArXiv, abs/1802.09477. https://arxiv.org/abs/1802.09477 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9","title":"Experiment results"}]}